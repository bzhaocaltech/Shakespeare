{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Some of the methods for this problem were reused from homework 5\n",
    "\n",
    "import pickle\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Activation\n",
    "import numpy as np\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "letter_data = pickle.load(open(\"data/letter_data.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionaries to convert between letter and index\n",
    "letter_int = {}\n",
    "int_letter = {}\n",
    "i = 0\n",
    "for poem in letter_data:\n",
    "    for letter in poem:\n",
    "        if letter not in letter_int:\n",
    "            letter_int[letter] = i\n",
    "            int_letter[i] = letter\n",
    "            i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns one-hot-encoded feature representation of the specified word given\n",
    "# a dictionary mapping words to their one-hot-encoded index.\n",
    "def get_word_repr(letter_to_int, word):\n",
    "    unique_words = letter_to_int.keys()\n",
    "    # Return a vector that's zero everywhere besides the index corresponding to <word>\n",
    "    feature_representation = np.zeros(len(unique_words))\n",
    "    feature_representation[letter_to_int[word]] = 1\n",
    "    return feature_representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_traindata(word_list, word_to_index, window_size=40, skip = 3):\n",
    "    \"\"\"\n",
    "    Generates training data for Skipgram model (sort of).\n",
    "\n",
    "    Arguments:\n",
    "        word_list:     Sequential list of letters (strings).\n",
    "        word_to_index: Dictionary mapping words to their corresponding index\n",
    "                       in a one-hot-encoded representation of our corpus.\n",
    "\n",
    "        window_size:   Size of Skipgram window.\n",
    "        \n",
    "        skip:          Skip every skip characters \n",
    "\n",
    "    Returns:\n",
    "        (trainX, trainY):     A pair of matrices (trainX, trainY) containing training\n",
    "                              points (one-hot-encoded vectors representing individual words) and\n",
    "                              their corresponding labels (also one-hot-encoded vectors representing words).\n",
    "    \"\"\"\n",
    "    trainX = []\n",
    "    trainY = []\n",
    "    for i in range(window_size, len(word_list), skip):\n",
    "        curr_word = word_list[i]\n",
    "        curr_X = []\n",
    "        for j in range(-window_size, 0):\n",
    "            if j != 0 and i + j >= 0 and i + j < len(word_list):\n",
    "                adjacent_word = word_list[i + j]\n",
    "                curr_X.append(get_word_repr(word_to_index, adjacent_word))\n",
    "        trainX.append(curr_X)\n",
    "        trainY.append(get_word_repr(word_to_index, curr_word))\n",
    "        \n",
    "    return (np.array(trainX), np.array(trainY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the training set\n",
    "unit = True\n",
    "train_x = -1\n",
    "train_y = -1\n",
    "for poem in letter_data:\n",
    "    poem_train_x, poem_train_y = generate_traindata(poem, letter_int)\n",
    "    if unit:\n",
    "        train_x = poem_train_x\n",
    "        train_y = poem_train_y\n",
    "        unit = False\n",
    "    else:\n",
    "        train_x = np.concatenate((train_x, poem_train_x))\n",
    "        train_y = np.concatenate((train_y, poem_train_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 150)               113400    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 38)                5738      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 38)                0         \n",
      "=================================================================\n",
      "Total params: 119,138\n",
      "Trainable params: 119,138\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/250\n",
      "29233/29233 [==============================] - 38s 1ms/step - loss: 2.9100 - acc: 0.1875\n",
      "Epoch 2/250\n",
      "29233/29233 [==============================] - 26s 895us/step - loss: 2.4828 - acc: 0.2994\n",
      "Epoch 3/250\n",
      "29233/29233 [==============================] - 23s 777us/step - loss: 2.2926 - acc: 0.3363\n",
      "Epoch 4/250\n",
      "29233/29233 [==============================] - 35s 1ms/step - loss: 2.1934 - acc: 0.3588\n",
      "Epoch 5/250\n",
      "29233/29233 [==============================] - 27s 930us/step - loss: 2.1153 - acc: 0.3806\n",
      "Epoch 6/250\n",
      "29233/29233 [==============================] - 31s 1ms/step - loss: 2.0458 - acc: 0.3993\n",
      "Epoch 7/250\n",
      "29233/29233 [==============================] - 32s 1ms/step - loss: 1.9875 - acc: 0.4133\n",
      "Epoch 8/250\n",
      "29233/29233 [==============================] - 28s 955us/step - loss: 1.9416 - acc: 0.4235\n",
      "Epoch 9/250\n",
      "29233/29233 [==============================] - 30s 1ms/step - loss: 1.8996 - acc: 0.4346\n",
      "Epoch 10/250\n",
      "29233/29233 [==============================] - 37s 1ms/step - loss: 1.8600 - acc: 0.4460\n",
      "Epoch 11/250\n",
      "29233/29233 [==============================] - 37s 1ms/step - loss: 1.8236 - acc: 0.4550\n",
      "Epoch 12/250\n",
      "29233/29233 [==============================] - 37s 1ms/step - loss: 1.7901 - acc: 0.4651\n",
      "Epoch 13/250\n",
      "29233/29233 [==============================] - 37s 1ms/step - loss: 1.7570 - acc: 0.4745\n",
      "Epoch 14/250\n",
      "29233/29233 [==============================] - 37s 1ms/step - loss: 1.7243 - acc: 0.4837\n",
      "Epoch 15/250\n",
      "29233/29233 [==============================] - 35s 1ms/step - loss: 1.6919 - acc: 0.4898\n",
      "Epoch 16/250\n",
      "29233/29233 [==============================] - 37s 1ms/step - loss: 1.6583 - acc: 0.4998\n",
      "Epoch 17/250\n",
      "29233/29233 [==============================] - 37s 1ms/step - loss: 1.6254 - acc: 0.5062\n",
      "Epoch 18/250\n",
      "29233/29233 [==============================] - 37s 1ms/step - loss: 1.5930 - acc: 0.5151\n",
      "Epoch 19/250\n",
      "29233/29233 [==============================] - 37s 1ms/step - loss: 1.5591 - acc: 0.5282\n",
      "Epoch 20/250\n",
      "29233/29233 [==============================] - 37s 1ms/step - loss: 1.5236 - acc: 0.5359\n",
      "Epoch 21/250\n",
      "29233/29233 [==============================] - 37s 1ms/step - loss: 1.4882 - acc: 0.5457\n",
      "Epoch 22/250\n",
      "29233/29233 [==============================] - 37s 1ms/step - loss: 1.4528 - acc: 0.5580\n",
      "Epoch 23/250\n",
      "29233/29233 [==============================] - 37s 1ms/step - loss: 1.4152 - acc: 0.5685\n",
      "Epoch 24/250\n",
      "29233/29233 [==============================] - 38s 1ms/step - loss: 1.3747 - acc: 0.5807\n",
      "Epoch 25/250\n",
      "29233/29233 [==============================] - 37s 1ms/step - loss: 1.3366 - acc: 0.5907\n",
      "Epoch 26/250\n",
      "29233/29233 [==============================] - 30s 1ms/step - loss: 1.2955 - acc: 0.6041\n",
      "Epoch 27/250\n",
      "29233/29233 [==============================] - 28s 968us/step - loss: 1.2549 - acc: 0.6173\n",
      "Epoch 28/250\n",
      "29233/29233 [==============================] - 22s 754us/step - loss: 1.2116 - acc: 0.6331\n",
      "Epoch 29/250\n",
      "29233/29233 [==============================] - 23s 802us/step - loss: 1.1698 - acc: 0.6457\n",
      "Epoch 30/250\n",
      "29233/29233 [==============================] - 21s 722us/step - loss: 1.1299 - acc: 0.6584\n",
      "Epoch 31/250\n",
      "29233/29233 [==============================] - 21s 729us/step - loss: 1.0869 - acc: 0.6725\n",
      "Epoch 32/250\n",
      "29233/29233 [==============================] - 21s 720us/step - loss: 1.0447 - acc: 0.6856\n",
      "Epoch 33/250\n",
      "29233/29233 [==============================] - 21s 720us/step - loss: 1.0018 - acc: 0.6997\n",
      "Epoch 34/250\n",
      "29233/29233 [==============================] - 21s 717us/step - loss: 0.9633 - acc: 0.7127\n",
      "Epoch 35/250\n",
      "29233/29233 [==============================] - 21s 717us/step - loss: 0.9199 - acc: 0.7266\n",
      "Epoch 36/250\n",
      "29233/29233 [==============================] - 21s 719us/step - loss: 0.8830 - acc: 0.7402\n",
      "Epoch 37/250\n",
      "29233/29233 [==============================] - 21s 718us/step - loss: 0.8403 - acc: 0.7539\n",
      "Epoch 38/250\n",
      "29233/29233 [==============================] - 21s 721us/step - loss: 0.8039 - acc: 0.7645\n",
      "Epoch 39/250\n",
      "29233/29233 [==============================] - 21s 720us/step - loss: 0.7672 - acc: 0.7754\n",
      "Epoch 40/250\n",
      "29233/29233 [==============================] - 21s 719us/step - loss: 0.7332 - acc: 0.7898\n",
      "Epoch 41/250\n",
      "29233/29233 [==============================] - 21s 725us/step - loss: 0.6944 - acc: 0.8007\n",
      "Epoch 42/250\n",
      "29233/29233 [==============================] - 21s 728us/step - loss: 0.6639 - acc: 0.8113\n",
      "Epoch 43/250\n",
      "29233/29233 [==============================] - 21s 723us/step - loss: 0.6336 - acc: 0.8222\n",
      "Epoch 44/250\n",
      "29233/29233 [==============================] - 21s 729us/step - loss: 0.6058 - acc: 0.8329\n",
      "Epoch 45/250\n",
      "29233/29233 [==============================] - 21s 725us/step - loss: 0.5726 - acc: 0.8410\n",
      "Epoch 46/250\n",
      "29233/29233 [==============================] - 21s 723us/step - loss: 0.5454 - acc: 0.8525\n",
      "Epoch 47/250\n",
      "29233/29233 [==============================] - 20s 690us/step - loss: 0.5217 - acc: 0.8575\n",
      "Epoch 48/250\n",
      "29233/29233 [==============================] - 20s 686us/step - loss: 0.4941 - acc: 0.8698\n",
      "Epoch 49/250\n",
      "29233/29233 [==============================] - 20s 688us/step - loss: 0.4726 - acc: 0.8750\n",
      "Epoch 50/250\n",
      "29233/29233 [==============================] - 20s 685us/step - loss: 0.4514 - acc: 0.8813\n",
      "Epoch 51/250\n",
      "29233/29233 [==============================] - 20s 685us/step - loss: 0.4333 - acc: 0.8843\n",
      "Epoch 52/250\n",
      "29233/29233 [==============================] - 20s 686us/step - loss: 0.4116 - acc: 0.8943\n",
      "Epoch 53/250\n",
      "29233/29233 [==============================] - 20s 685us/step - loss: 0.3971 - acc: 0.8967\n",
      "Epoch 54/250\n",
      "29233/29233 [==============================] - 20s 681us/step - loss: 0.3774 - acc: 0.9036\n",
      "Epoch 55/250\n",
      "29233/29233 [==============================] - 20s 684us/step - loss: 0.3633 - acc: 0.9086\n",
      "Epoch 56/250\n",
      "29233/29233 [==============================] - 20s 685us/step - loss: 0.3476 - acc: 0.9129\n",
      "Epoch 57/250\n",
      "29233/29233 [==============================] - 20s 683us/step - loss: 0.3295 - acc: 0.9191\n",
      "Epoch 58/250\n",
      "29233/29233 [==============================] - 20s 691us/step - loss: 0.3204 - acc: 0.9193\n",
      "Epoch 59/250\n",
      "29233/29233 [==============================] - 20s 679us/step - loss: 0.3100 - acc: 0.9243\n",
      "Epoch 60/250\n",
      "29233/29233 [==============================] - 20s 684us/step - loss: 0.2966 - acc: 0.9271\n",
      "Epoch 61/250\n",
      "29233/29233 [==============================] - 20s 684us/step - loss: 0.2828 - acc: 0.9328\n",
      "Epoch 62/250\n",
      "29233/29233 [==============================] - 20s 681us/step - loss: 0.2734 - acc: 0.9352\n",
      "Epoch 63/250\n",
      "29233/29233 [==============================] - 20s 684us/step - loss: 0.2655 - acc: 0.9382\n",
      "Epoch 64/250\n",
      "29233/29233 [==============================] - 20s 686us/step - loss: 0.2582 - acc: 0.9385\n",
      "Epoch 65/250\n",
      "29233/29233 [==============================] - 20s 691us/step - loss: 0.2497 - acc: 0.9409\n",
      "Epoch 66/250\n",
      "29233/29233 [==============================] - 20s 684us/step - loss: 0.2414 - acc: 0.9426\n",
      "Epoch 67/250\n",
      "29233/29233 [==============================] - 20s 691us/step - loss: 0.2344 - acc: 0.9443\n",
      "Epoch 68/250\n",
      "29233/29233 [==============================] - 20s 682us/step - loss: 0.2257 - acc: 0.9479\n",
      "Epoch 69/250\n",
      "29233/29233 [==============================] - 20s 686us/step - loss: 0.2208 - acc: 0.9489\n",
      "Epoch 70/250\n",
      "29233/29233 [==============================] - 20s 687us/step - loss: 0.2105 - acc: 0.9506\n",
      "Epoch 71/250\n",
      "29233/29233 [==============================] - 20s 682us/step - loss: 0.2086 - acc: 0.9504\n",
      "Epoch 72/250\n",
      "29233/29233 [==============================] - 20s 682us/step - loss: 0.2031 - acc: 0.9522\n",
      "Epoch 73/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29233/29233 [==============================] - 20s 675us/step - loss: 0.1951 - acc: 0.9546\n",
      "Epoch 74/250\n",
      "29233/29233 [==============================] - 20s 676us/step - loss: 0.1894 - acc: 0.9574\n",
      "Epoch 75/250\n",
      "29233/29233 [==============================] - 20s 675us/step - loss: 0.1857 - acc: 0.9580\n",
      "Epoch 76/250\n",
      "29233/29233 [==============================] - 20s 674us/step - loss: 0.1839 - acc: 0.9589\n",
      "Epoch 77/250\n",
      "29233/29233 [==============================] - 20s 674us/step - loss: 0.1773 - acc: 0.9593\n",
      "Epoch 78/250\n",
      "29233/29233 [==============================] - 20s 674us/step - loss: 0.1757 - acc: 0.9587\n",
      "Epoch 79/250\n",
      "29233/29233 [==============================] - 20s 681us/step - loss: 0.1696 - acc: 0.9617\n",
      "Epoch 80/250\n",
      "29233/29233 [==============================] - 20s 675us/step - loss: 0.1690 - acc: 0.9615\n",
      "Epoch 81/250\n",
      "29233/29233 [==============================] - 20s 675us/step - loss: 0.1654 - acc: 0.9618\n",
      "Epoch 82/250\n",
      "29233/29233 [==============================] - 20s 673us/step - loss: 0.1614 - acc: 0.9626\n",
      "Epoch 83/250\n",
      "29233/29233 [==============================] - 20s 672us/step - loss: 0.1559 - acc: 0.9656\n",
      "Epoch 84/250\n",
      "29233/29233 [==============================] - 20s 675us/step - loss: 0.1549 - acc: 0.9648\n",
      "Epoch 85/250\n",
      "29233/29233 [==============================] - 20s 676us/step - loss: 0.1540 - acc: 0.9644\n",
      "Epoch 86/250\n",
      "29233/29233 [==============================] - 20s 678us/step - loss: 0.1551 - acc: 0.9640\n",
      "Epoch 87/250\n",
      "29233/29233 [==============================] - 20s 676us/step - loss: 0.1499 - acc: 0.9653\n",
      "Epoch 88/250\n",
      "29233/29233 [==============================] - 20s 676us/step - loss: 0.1506 - acc: 0.9642\n",
      "Epoch 89/250\n",
      "29233/29233 [==============================] - 20s 678us/step - loss: 0.1463 - acc: 0.9661\n",
      "Epoch 90/250\n",
      "29233/29233 [==============================] - 20s 677us/step - loss: 0.1433 - acc: 0.9670\n",
      "Epoch 91/250\n",
      "29233/29233 [==============================] - 20s 677us/step - loss: 0.1377 - acc: 0.9684\n",
      "Epoch 92/250\n",
      "29233/29233 [==============================] - 20s 675us/step - loss: 0.1395 - acc: 0.9666\n",
      "Epoch 93/250\n",
      "29233/29233 [==============================] - 20s 677us/step - loss: 0.1430 - acc: 0.9645\n",
      "Epoch 94/250\n",
      "29233/29233 [==============================] - 20s 676us/step - loss: 0.1304 - acc: 0.9713\n",
      "Epoch 95/250\n",
      "29233/29233 [==============================] - 20s 675us/step - loss: 0.1344 - acc: 0.9673\n",
      "Epoch 96/250\n",
      "29233/29233 [==============================] - 20s 676us/step - loss: 0.1344 - acc: 0.9678\n",
      "Epoch 97/250\n",
      "29233/29233 [==============================] - 20s 677us/step - loss: 0.1305 - acc: 0.9685\n",
      "Epoch 98/250\n",
      "29233/29233 [==============================] - 20s 677us/step - loss: 0.1316 - acc: 0.9679\n",
      "Epoch 99/250\n",
      "29233/29233 [==============================] - 20s 676us/step - loss: 0.1279 - acc: 0.9699\n",
      "Epoch 100/250\n",
      "29233/29233 [==============================] - 20s 677us/step - loss: 0.1262 - acc: 0.9700\n",
      "Epoch 101/250\n",
      "29233/29233 [==============================] - 20s 677us/step - loss: 0.1242 - acc: 0.9711\n",
      "Epoch 102/250\n",
      "29233/29233 [==============================] - 20s 677us/step - loss: 0.1208 - acc: 0.9722\n",
      "Epoch 103/250\n",
      "29233/29233 [==============================] - 20s 678us/step - loss: 0.1217 - acc: 0.9712\n",
      "Epoch 104/250\n",
      "29233/29233 [==============================] - 20s 677us/step - loss: 0.1241 - acc: 0.9700\n",
      "Epoch 105/250\n",
      "29233/29233 [==============================] - 20s 676us/step - loss: 0.1190 - acc: 0.9709\n",
      "Epoch 106/250\n",
      "29233/29233 [==============================] - 20s 674us/step - loss: 0.1185 - acc: 0.9715\n",
      "Epoch 107/250\n",
      "29233/29233 [==============================] - 20s 676us/step - loss: 0.1173 - acc: 0.9717\n",
      "Epoch 108/250\n",
      "29233/29233 [==============================] - 20s 678us/step - loss: 0.1159 - acc: 0.9725\n",
      "Epoch 109/250\n",
      "29233/29233 [==============================] - 20s 675us/step - loss: 0.1165 - acc: 0.9712\n",
      "Epoch 110/250\n",
      "29233/29233 [==============================] - 20s 678us/step - loss: 0.1151 - acc: 0.9722\n",
      "Epoch 111/250\n",
      "29233/29233 [==============================] - 20s 676us/step - loss: 0.1127 - acc: 0.9722\n",
      "Epoch 112/250\n",
      "29233/29233 [==============================] - 20s 676us/step - loss: 0.1106 - acc: 0.9741\n",
      "Epoch 113/250\n",
      "29233/29233 [==============================] - 20s 681us/step - loss: 0.1129 - acc: 0.9719\n",
      "Epoch 114/250\n",
      "29233/29233 [==============================] - 20s 679us/step - loss: 0.1100 - acc: 0.9729\n",
      "Epoch 115/250\n",
      "29233/29233 [==============================] - 20s 678us/step - loss: 0.1085 - acc: 0.9732\n",
      "Epoch 116/250\n",
      "29233/29233 [==============================] - 20s 680us/step - loss: 0.1085 - acc: 0.9736\n",
      "Epoch 117/250\n",
      "29233/29233 [==============================] - 20s 677us/step - loss: 0.1070 - acc: 0.9723\n",
      "Epoch 118/250\n",
      "29233/29233 [==============================] - 20s 678us/step - loss: 0.1095 - acc: 0.9730\n",
      "Epoch 119/250\n",
      "29233/29233 [==============================] - 20s 679us/step - loss: 0.1048 - acc: 0.9742\n",
      "Epoch 120/250\n",
      "29233/29233 [==============================] - 20s 677us/step - loss: 0.1052 - acc: 0.9745\n",
      "Epoch 121/250\n",
      "29233/29233 [==============================] - 20s 679us/step - loss: 0.1030 - acc: 0.9743\n",
      "Epoch 122/250\n",
      "29233/29233 [==============================] - 20s 679us/step - loss: 0.1035 - acc: 0.9734\n",
      "Epoch 123/250\n",
      "29233/29233 [==============================] - 20s 676us/step - loss: 0.1064 - acc: 0.9731\n",
      "Epoch 124/250\n",
      "29233/29233 [==============================] - 20s 676us/step - loss: 0.1074 - acc: 0.9738\n",
      "Epoch 125/250\n",
      "29233/29233 [==============================] - 20s 676us/step - loss: 0.1056 - acc: 0.9723\n",
      "Epoch 126/250\n",
      "29233/29233 [==============================] - 20s 675us/step - loss: 0.0995 - acc: 0.9757\n",
      "Epoch 127/250\n",
      "29233/29233 [==============================] - 20s 677us/step - loss: 0.1042 - acc: 0.9738\n",
      "Epoch 128/250\n",
      "29233/29233 [==============================] - 20s 679us/step - loss: 0.0999 - acc: 0.9756\n",
      "Epoch 129/250\n",
      "29233/29233 [==============================] - 20s 679us/step - loss: 0.0985 - acc: 0.9755\n",
      "Epoch 130/250\n",
      "29233/29233 [==============================] - 20s 679us/step - loss: 0.0960 - acc: 0.9758\n",
      "Epoch 131/250\n",
      "29233/29233 [==============================] - 20s 687us/step - loss: 0.0973 - acc: 0.9756\n",
      "Epoch 132/250\n",
      "29233/29233 [==============================] - 23s 774us/step - loss: 0.1024 - acc: 0.9733\n",
      "Epoch 133/250\n",
      "29233/29233 [==============================] - 32s 1ms/step - loss: 0.0961 - acc: 0.9755\n",
      "Epoch 134/250\n",
      "29233/29233 [==============================] - 38s 1ms/step - loss: 0.0959 - acc: 0.9761\n",
      "Epoch 135/250\n",
      "29233/29233 [==============================] - 38s 1ms/step - loss: 0.0958 - acc: 0.9758\n",
      "Epoch 136/250\n",
      "29233/29233 [==============================] - 37s 1ms/step - loss: 0.0994 - acc: 0.9740\n",
      "Epoch 137/250\n",
      "29233/29233 [==============================] - 37s 1ms/step - loss: 0.0939 - acc: 0.9757\n",
      "Epoch 138/250\n",
      "29233/29233 [==============================] - 34s 1ms/step - loss: 0.0985 - acc: 0.9747\n",
      "Epoch 139/250\n",
      "29233/29233 [==============================] - 36s 1ms/step - loss: 0.0915 - acc: 0.9763\n",
      "Epoch 140/250\n",
      "29233/29233 [==============================] - 37s 1ms/step - loss: 0.0951 - acc: 0.9745\n",
      "Epoch 141/250\n",
      "29233/29233 [==============================] - 29s 1ms/step - loss: 0.0908 - acc: 0.9778\n",
      "Epoch 142/250\n",
      "29233/29233 [==============================] - 23s 776us/step - loss: 0.0917 - acc: 0.9762\n",
      "Epoch 143/250\n",
      "29233/29233 [==============================] - 27s 918us/step - loss: 0.0939 - acc: 0.9756\n",
      "Epoch 144/250\n",
      "29233/29233 [==============================] - 24s 808us/step - loss: 0.0928 - acc: 0.9760\n",
      "Epoch 145/250\n",
      "29233/29233 [==============================] - 27s 927us/step - loss: 0.0907 - acc: 0.9775\n",
      "Epoch 146/250\n",
      "29233/29233 [==============================] - 36s 1ms/step - loss: 0.0882 - acc: 0.9775\n",
      "Epoch 147/250\n",
      "29233/29233 [==============================] - 39s 1ms/step - loss: 0.0883 - acc: 0.9774\n",
      "Epoch 148/250\n",
      "29233/29233 [==============================] - 38s 1ms/step - loss: 0.0922 - acc: 0.9764\n",
      "Epoch 149/250\n",
      "29233/29233 [==============================] - 38s 1ms/step - loss: 0.0889 - acc: 0.9764\n",
      "Epoch 150/250\n",
      "29233/29233 [==============================] - 33s 1ms/step - loss: 0.0923 - acc: 0.9760\n",
      "Epoch 151/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29233/29233 [==============================] - 37s 1ms/step - loss: 0.0859 - acc: 0.9778\n",
      "Epoch 152/250\n",
      "29233/29233 [==============================] - 34s 1ms/step - loss: 0.0888 - acc: 0.9774\n",
      "Epoch 153/250\n",
      "29233/29233 [==============================] - 21s 721us/step - loss: 0.0921 - acc: 0.9761\n",
      "Epoch 154/250\n",
      "29233/29233 [==============================] - 34s 1ms/step - loss: 0.0870 - acc: 0.9781\n",
      "Epoch 155/250\n",
      "29233/29233 [==============================] - 37s 1ms/step - loss: 0.0859 - acc: 0.9776\n",
      "Epoch 156/250\n",
      "29233/29233 [==============================] - 37s 1ms/step - loss: 0.0858 - acc: 0.9775\n",
      "Epoch 157/250\n",
      "29233/29233 [==============================] - 37s 1ms/step - loss: 0.0877 - acc: 0.9770\n",
      "Epoch 158/250\n",
      "29233/29233 [==============================] - 38s 1ms/step - loss: 0.0848 - acc: 0.9787\n",
      "Epoch 159/250\n",
      "29233/29233 [==============================] - 37s 1ms/step - loss: 0.0836 - acc: 0.9782\n",
      "Epoch 160/250\n",
      "29233/29233 [==============================] - 37s 1ms/step - loss: 0.0833 - acc: 0.9781\n",
      "Epoch 161/250\n",
      "29233/29233 [==============================] - 36s 1ms/step - loss: 0.0850 - acc: 0.9778\n",
      "Epoch 162/250\n",
      "29233/29233 [==============================] - 35s 1ms/step - loss: 0.0858 - acc: 0.9778\n",
      "Epoch 163/250\n",
      "29233/29233 [==============================] - 36s 1ms/step - loss: 0.0844 - acc: 0.9779\n",
      "Epoch 164/250\n",
      "29233/29233 [==============================] - 36s 1ms/step - loss: 0.0844 - acc: 0.9774\n",
      "Epoch 165/250\n",
      "29233/29233 [==============================] - 36s 1ms/step - loss: 0.0835 - acc: 0.9776\n",
      "Epoch 166/250\n",
      "29233/29233 [==============================] - 28s 943us/step - loss: 0.0824 - acc: 0.9781\n",
      "Epoch 167/250\n",
      "29233/29233 [==============================] - 26s 884us/step - loss: 0.0807 - acc: 0.9786\n",
      "Epoch 168/250\n",
      "29233/29233 [==============================] - 25s 865us/step - loss: 0.0834 - acc: 0.9772\n",
      "Epoch 169/250\n",
      "29233/29233 [==============================] - 25s 859us/step - loss: 0.0830 - acc: 0.9777\n",
      "Epoch 170/250\n",
      "29233/29233 [==============================] - 26s 879us/step - loss: 0.0851 - acc: 0.9780\n",
      "Epoch 171/250\n",
      "29233/29233 [==============================] - 27s 920us/step - loss: 0.0780 - acc: 0.9797\n",
      "Epoch 172/250\n",
      "29233/29233 [==============================] - 41s 1ms/step - loss: 0.0794 - acc: 0.9800\n",
      "Epoch 173/250\n",
      "29233/29233 [==============================] - 33s 1ms/step - loss: 0.0810 - acc: 0.9791\n",
      "Epoch 174/250\n",
      "29233/29233 [==============================] - 36s 1ms/step - loss: 0.0807 - acc: 0.9795\n",
      "Epoch 175/250\n",
      "29233/29233 [==============================] - 38s 1ms/step - loss: 0.0812 - acc: 0.9780\n",
      "Epoch 176/250\n",
      "29233/29233 [==============================] - 37s 1ms/step - loss: 0.0827 - acc: 0.9776\n",
      "Epoch 177/250\n",
      "29233/29233 [==============================] - 38s 1ms/step - loss: 0.0799 - acc: 0.9787\n",
      "Epoch 178/250\n",
      "29233/29233 [==============================] - 38s 1ms/step - loss: 0.0837 - acc: 0.9768\n",
      "Epoch 179/250\n",
      "29233/29233 [==============================] - 37s 1ms/step - loss: 0.0791 - acc: 0.9789\n",
      "Epoch 180/250\n",
      "29233/29233 [==============================] - 37s 1ms/step - loss: 0.0763 - acc: 0.9803\n",
      "Epoch 181/250\n",
      "29233/29233 [==============================] - 37s 1ms/step - loss: 0.0801 - acc: 0.9788\n",
      "Epoch 182/250\n",
      "29233/29233 [==============================] - 38s 1ms/step - loss: 0.0805 - acc: 0.9786\n",
      "Epoch 183/250\n",
      "29233/29233 [==============================] - 38s 1ms/step - loss: 0.0747 - acc: 0.9796\n",
      "Epoch 184/250\n",
      "29233/29233 [==============================] - 37s 1ms/step - loss: 0.0768 - acc: 0.9803\n",
      "Epoch 185/250\n",
      "29233/29233 [==============================] - 38s 1ms/step - loss: 0.0755 - acc: 0.9797\n",
      "Epoch 186/250\n",
      "29233/29233 [==============================] - 37s 1ms/step - loss: 0.0789 - acc: 0.9781\n",
      "Epoch 187/250\n",
      "29233/29233 [==============================] - 26s 875us/step - loss: 0.0754 - acc: 0.9800\n",
      "Epoch 188/250\n",
      "29233/29233 [==============================] - 26s 879us/step - loss: 0.0756 - acc: 0.9801\n",
      "Epoch 189/250\n",
      "29233/29233 [==============================] - 36s 1ms/step - loss: 0.0748 - acc: 0.9807\n",
      "Epoch 190/250\n",
      "29233/29233 [==============================] - 38s 1ms/step - loss: 0.0764 - acc: 0.9786\n",
      "Epoch 191/250\n",
      "29233/29233 [==============================] - 38s 1ms/step - loss: 0.0746 - acc: 0.9796\n",
      "Epoch 192/250\n",
      "29233/29233 [==============================] - 35s 1ms/step - loss: 0.0715 - acc: 0.9812\n",
      "Epoch 193/250\n",
      "29233/29233 [==============================] - 43s 1ms/step - loss: 0.0784 - acc: 0.9780\n",
      "Epoch 194/250\n",
      "29233/29233 [==============================] - 42s 1ms/step - loss: 0.0774 - acc: 0.9792\n",
      "Epoch 195/250\n",
      "29233/29233 [==============================] - 36s 1ms/step - loss: 0.0785 - acc: 0.9780\n",
      "Epoch 196/250\n",
      "29233/29233 [==============================] - 37s 1ms/step - loss: 0.0770 - acc: 0.9794\n",
      "Epoch 197/250\n",
      "29233/29233 [==============================] - 38s 1ms/step - loss: 0.0766 - acc: 0.9793\n",
      "Epoch 198/250\n",
      "29233/29233 [==============================] - 35s 1ms/step - loss: 0.0724 - acc: 0.9806\n",
      "Epoch 199/250\n",
      "29233/29233 [==============================] - 38s 1ms/step - loss: 0.0712 - acc: 0.9812\n",
      "Epoch 200/250\n",
      "29233/29233 [==============================] - 38s 1ms/step - loss: 0.0746 - acc: 0.9797\n",
      "Epoch 201/250\n",
      "29233/29233 [==============================] - 39s 1ms/step - loss: 0.0737 - acc: 0.9794\n",
      "Epoch 202/250\n",
      "29233/29233 [==============================] - 37s 1ms/step - loss: 0.0720 - acc: 0.9810\n",
      "Epoch 203/250\n",
      "29233/29233 [==============================] - 38s 1ms/step - loss: 0.0793 - acc: 0.9776\n",
      "Epoch 204/250\n",
      "29233/29233 [==============================] - 37s 1ms/step - loss: 0.0708 - acc: 0.9810\n",
      "Epoch 205/250\n",
      "29233/29233 [==============================] - 28s 970us/step - loss: 0.0718 - acc: 0.9813\n",
      "Epoch 206/250\n",
      "29233/29233 [==============================] - 36s 1ms/step - loss: 0.0716 - acc: 0.9791\n",
      "Epoch 207/250\n",
      "29233/29233 [==============================] - 22s 766us/step - loss: 0.0742 - acc: 0.9788\n",
      "Epoch 208/250\n",
      "29233/29233 [==============================] - 21s 716us/step - loss: 0.0720 - acc: 0.9804\n",
      "Epoch 209/250\n",
      "29233/29233 [==============================] - 21s 708us/step - loss: 0.0718 - acc: 0.9808\n",
      "Epoch 210/250\n",
      "29233/29233 [==============================] - 34s 1ms/step - loss: 0.0681 - acc: 0.9820\n",
      "Epoch 211/250\n",
      "29233/29233 [==============================] - 29s 978us/step - loss: 0.0722 - acc: 0.9808\n",
      "Epoch 212/250\n",
      "29233/29233 [==============================] - 22s 764us/step - loss: 0.0681 - acc: 0.9821\n",
      "Epoch 213/250\n",
      "29233/29233 [==============================] - 23s 781us/step - loss: 0.0746 - acc: 0.9796\n",
      "Epoch 214/250\n",
      "29233/29233 [==============================] - 29s 1ms/step - loss: 0.0704 - acc: 0.9804\n",
      "Epoch 215/250\n",
      "29233/29233 [==============================] - 38s 1ms/step - loss: 0.0692 - acc: 0.9810\n",
      "Epoch 216/250\n",
      "29233/29233 [==============================] - 38s 1ms/step - loss: 0.0715 - acc: 0.9809\n",
      "Epoch 217/250\n",
      "29233/29233 [==============================] - 39s 1ms/step - loss: 0.0705 - acc: 0.9815\n",
      "Epoch 218/250\n",
      "29233/29233 [==============================] - 38s 1ms/step - loss: 0.0686 - acc: 0.9818\n",
      "Epoch 219/250\n",
      "29233/29233 [==============================] - 38s 1ms/step - loss: 0.0687 - acc: 0.9811\n",
      "Epoch 220/250\n",
      "29233/29233 [==============================] - 38s 1ms/step - loss: 0.0685 - acc: 0.9820\n",
      "Epoch 221/250\n",
      "29233/29233 [==============================] - 31s 1ms/step - loss: 0.0668 - acc: 0.9820\n",
      "Epoch 222/250\n",
      "29233/29233 [==============================] - 24s 826us/step - loss: 0.0709 - acc: 0.9806\n",
      "Epoch 223/250\n",
      "29233/29233 [==============================] - 21s 707us/step - loss: 0.0681 - acc: 0.9819\n",
      "Epoch 224/250\n",
      "29233/29233 [==============================] - 21s 722us/step - loss: 0.0673 - acc: 0.9822\n",
      "Epoch 225/250\n",
      "29233/29233 [==============================] - 23s 793us/step - loss: 0.0675 - acc: 0.9821\n",
      "Epoch 226/250\n",
      "29233/29233 [==============================] - 21s 722us/step - loss: 0.0711 - acc: 0.9804\n",
      "Epoch 227/250\n",
      "29233/29233 [==============================] - 20s 667us/step - loss: 0.0667 - acc: 0.9820\n",
      "Epoch 228/250\n",
      "29233/29233 [==============================] - 21s 718us/step - loss: 0.0677 - acc: 0.9816\n",
      "Epoch 229/250\n",
      "29233/29233 [==============================] - 21s 716us/step - loss: 0.0655 - acc: 0.9826\n",
      "Epoch 230/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29233/29233 [==============================] - 22s 761us/step - loss: 0.0628 - acc: 0.9830\n",
      "Epoch 231/250\n",
      "29233/29233 [==============================] - 24s 834us/step - loss: 0.0668 - acc: 0.9817\n",
      "Epoch 232/250\n",
      "29233/29233 [==============================] - 21s 732us/step - loss: 0.0654 - acc: 0.9812\n",
      "Epoch 233/250\n",
      "29233/29233 [==============================] - 22s 737us/step - loss: 0.0632 - acc: 0.9836\n",
      "Epoch 234/250\n",
      "29233/29233 [==============================] - 22s 765us/step - loss: 0.0684 - acc: 0.9816\n",
      "Epoch 235/250\n",
      "29233/29233 [==============================] - 22s 739us/step - loss: 0.0679 - acc: 0.9815\n",
      "Epoch 236/250\n",
      "29233/29233 [==============================] - 21s 723us/step - loss: 0.0617 - acc: 0.9831\n",
      "Epoch 237/250\n",
      "29233/29233 [==============================] - 22s 759us/step - loss: 0.0689 - acc: 0.9809\n",
      "Epoch 238/250\n",
      "29233/29233 [==============================] - 24s 834us/step - loss: 0.0638 - acc: 0.9822\n",
      "Epoch 239/250\n",
      "29233/29233 [==============================] - 21s 707us/step - loss: 0.0704 - acc: 0.9808\n",
      "Epoch 240/250\n",
      "29233/29233 [==============================] - 20s 699us/step - loss: 0.0649 - acc: 0.9816\n",
      "Epoch 241/250\n",
      "29233/29233 [==============================] - 21s 706us/step - loss: 0.0662 - acc: 0.9817\n",
      "Epoch 242/250\n",
      "29233/29233 [==============================] - 20s 688us/step - loss: 0.0662 - acc: 0.9807\n",
      "Epoch 243/250\n",
      "29233/29233 [==============================] - 21s 706us/step - loss: 0.0690 - acc: 0.9809\n",
      "Epoch 244/250\n",
      "29233/29233 [==============================] - 20s 698us/step - loss: 0.0664 - acc: 0.9814\n",
      "Epoch 245/250\n",
      "29233/29233 [==============================] - 22s 739us/step - loss: 0.0656 - acc: 0.9829\n",
      "Epoch 246/250\n",
      "29233/29233 [==============================] - 20s 686us/step - loss: 0.0675 - acc: 0.9817\n",
      "Epoch 247/250\n",
      "29233/29233 [==============================] - 20s 686us/step - loss: 0.0625 - acc: 0.9829\n",
      "Epoch 248/250\n",
      "29233/29233 [==============================] - 20s 695us/step - loss: 0.0635 - acc: 0.9831\n",
      "Epoch 249/250\n",
      "29233/29233 [==============================] - 21s 734us/step - loss: 0.0642 - acc: 0.9826\n",
      "Epoch 250/250\n",
      "29233/29233 [==============================] - 21s 722us/step - loss: 0.0639 - acc: 0.9823\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f41f419feb8>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate the neural network\n",
    "model = Sequential()\n",
    "model.add(LSTM(150, input_shape = (len(train_x[0]), len(train_x[0][0]))))\n",
    "model.add(Dense(len(train_y[0])))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss = \"categorical_crossentropy\", optimizer = \"rmsprop\", metrics = ['accuracy'])\n",
    "model.fit(train_x, train_y, epochs = 250, batch_size = 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "29233/29233 [==============================] - 24s 830us/step - loss: 0.0960 - acc: 0.9702\n",
      "Epoch 2/100\n",
      "29233/29233 [==============================] - 25s 839us/step - loss: 0.0927 - acc: 0.9717\n",
      "Epoch 3/100\n",
      "29233/29233 [==============================] - 25s 857us/step - loss: 0.0951 - acc: 0.9707\n",
      "Epoch 4/100\n",
      "29233/29233 [==============================] - 25s 852us/step - loss: 0.0926 - acc: 0.9716\n",
      "Epoch 5/100\n",
      "29233/29233 [==============================] - 25s 862us/step - loss: 0.0930 - acc: 0.9719\n",
      "Epoch 6/100\n",
      "29233/29233 [==============================] - 25s 872us/step - loss: 0.0942 - acc: 0.9717\n",
      "Epoch 7/100\n",
      "29233/29233 [==============================] - 25s 848us/step - loss: 0.0903 - acc: 0.9736\n",
      "Epoch 8/100\n",
      "29233/29233 [==============================] - 24s 832us/step - loss: 0.0927 - acc: 0.9711\n",
      "Epoch 9/100\n",
      "29233/29233 [==============================] - 25s 853us/step - loss: 0.0977 - acc: 0.9703\n",
      "Epoch 10/100\n",
      "29233/29233 [==============================] - 25s 866us/step - loss: 0.0900 - acc: 0.9737\n",
      "Epoch 11/100\n",
      "29233/29233 [==============================] - 25s 856us/step - loss: 0.0885 - acc: 0.9733\n",
      "Epoch 12/100\n",
      "29233/29233 [==============================] - 24s 825us/step - loss: 0.0913 - acc: 0.9715\n",
      "Epoch 13/100\n",
      "29233/29233 [==============================] - 25s 845us/step - loss: 0.0882 - acc: 0.9738\n",
      "Epoch 14/100\n",
      "29233/29233 [==============================] - 25s 850us/step - loss: 0.0957 - acc: 0.9711\n",
      "Epoch 15/100\n",
      "29233/29233 [==============================] - 39s 1ms/step - loss: 0.0910 - acc: 0.9722\n",
      "Epoch 16/100\n",
      "29233/29233 [==============================] - 43s 1ms/step - loss: 0.0936 - acc: 0.9710\n",
      "Epoch 17/100\n",
      "29233/29233 [==============================] - 43s 1ms/step - loss: 0.0917 - acc: 0.9722\n",
      "Epoch 18/100\n",
      "29233/29233 [==============================] - 43s 1ms/step - loss: 0.0888 - acc: 0.9729\n",
      "Epoch 19/100\n",
      "29233/29233 [==============================] - 42s 1ms/step - loss: 0.0917 - acc: 0.9717\n",
      "Epoch 20/100\n",
      "29233/29233 [==============================] - 42s 1ms/step - loss: 0.0894 - acc: 0.9727\n",
      "Epoch 21/100\n",
      "29233/29233 [==============================] - 29s 993us/step - loss: 0.0882 - acc: 0.9738\n",
      "Epoch 22/100\n",
      "29233/29233 [==============================] - 28s 948us/step - loss: 0.0876 - acc: 0.9742\n",
      "Epoch 23/100\n",
      "29233/29233 [==============================] - 28s 944us/step - loss: 0.0885 - acc: 0.9744\n",
      "Epoch 24/100\n",
      "29233/29233 [==============================] - 28s 942us/step - loss: 0.0911 - acc: 0.9715\n",
      "Epoch 25/100\n",
      "29233/29233 [==============================] - 27s 917us/step - loss: 0.0848 - acc: 0.9735\n",
      "Epoch 26/100\n",
      "29233/29233 [==============================] - 27s 919us/step - loss: 0.0904 - acc: 0.9720\n",
      "Epoch 27/100\n",
      "29233/29233 [==============================] - 27s 929us/step - loss: 0.0887 - acc: 0.9734\n",
      "Epoch 28/100\n",
      "29233/29233 [==============================] - 28s 949us/step - loss: 0.0872 - acc: 0.9731\n",
      "Epoch 29/100\n",
      "29233/29233 [==============================] - 30s 1ms/step - loss: 0.0881 - acc: 0.9729\n",
      "Epoch 30/100\n",
      "29233/29233 [==============================] - 45s 2ms/step - loss: 0.0821 - acc: 0.9770\n",
      "Epoch 31/100\n",
      "29233/29233 [==============================] - 49s 2ms/step - loss: 0.0822 - acc: 0.9756\n",
      "Epoch 32/100\n",
      "29233/29233 [==============================] - 42s 1ms/step - loss: 0.0817 - acc: 0.9742\n",
      "Epoch 33/100\n",
      "29233/29233 [==============================] - 43s 1ms/step - loss: 0.0860 - acc: 0.9742\n",
      "Epoch 34/100\n",
      "29233/29233 [==============================] - 28s 952us/step - loss: 0.0869 - acc: 0.9726\n",
      "Epoch 35/100\n",
      "29233/29233 [==============================] - 28s 961us/step - loss: 0.0892 - acc: 0.9722\n",
      "Epoch 36/100\n",
      "29233/29233 [==============================] - 28s 966us/step - loss: 0.0832 - acc: 0.9750\n",
      "Epoch 37/100\n",
      "29233/29233 [==============================] - 28s 964us/step - loss: 0.0898 - acc: 0.9730\n",
      "Epoch 38/100\n",
      "29233/29233 [==============================] - 27s 920us/step - loss: 0.0901 - acc: 0.9727\n",
      "Epoch 39/100\n",
      "29233/29233 [==============================] - 28s 967us/step - loss: 0.0848 - acc: 0.9742\n",
      "Epoch 40/100\n",
      "29233/29233 [==============================] - 26s 883us/step - loss: 0.0830 - acc: 0.9749\n",
      "Epoch 41/100\n",
      "29233/29233 [==============================] - 29s 975us/step - loss: 0.0859 - acc: 0.9747\n",
      "Epoch 42/100\n",
      "29233/29233 [==============================] - 28s 968us/step - loss: 0.0850 - acc: 0.9738\n",
      "Epoch 43/100\n",
      "29233/29233 [==============================] - 28s 948us/step - loss: 0.0841 - acc: 0.9743\n",
      "Epoch 44/100\n",
      "29233/29233 [==============================] - 24s 805us/step - loss: 0.0848 - acc: 0.9750\n",
      "Epoch 45/100\n",
      "29233/29233 [==============================] - 26s 896us/step - loss: 0.0867 - acc: 0.9735\n",
      "Epoch 46/100\n",
      "29233/29233 [==============================] - 24s 836us/step - loss: 0.0830 - acc: 0.9748\n",
      "Epoch 47/100\n",
      "29233/29233 [==============================] - 25s 869us/step - loss: 0.0851 - acc: 0.9736\n",
      "Epoch 48/100\n",
      "29233/29233 [==============================] - 24s 822us/step - loss: 0.0807 - acc: 0.9752\n",
      "Epoch 49/100\n",
      "29233/29233 [==============================] - 26s 874us/step - loss: 0.0816 - acc: 0.9754\n",
      "Epoch 50/100\n",
      "29233/29233 [==============================] - 27s 935us/step - loss: 0.0839 - acc: 0.9752\n",
      "Epoch 51/100\n",
      "29233/29233 [==============================] - 28s 962us/step - loss: 0.0808 - acc: 0.9763\n",
      "Epoch 52/100\n",
      "29233/29233 [==============================] - 24s 833us/step - loss: 0.0842 - acc: 0.9745\n",
      "Epoch 53/100\n",
      "29233/29233 [==============================] - 24s 829us/step - loss: 0.0778 - acc: 0.9765\n",
      "Epoch 54/100\n",
      "29233/29233 [==============================] - 23s 801us/step - loss: 0.0825 - acc: 0.9749\n",
      "Epoch 55/100\n",
      "29233/29233 [==============================] - 24s 829us/step - loss: 0.0778 - acc: 0.9764\n",
      "Epoch 56/100\n",
      "28544/29233 [============================>.] - ETA: 0s - loss: 0.0841 - acc: 0.9737"
     ]
    }
   ],
   "source": [
    "model.fit(train_x, train_y, epochs = 100, batch_size = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert a line to one hot form\n",
    "def convert_line_to_one_hot(line, letter_int):\n",
    "    output = []\n",
    "    for letter in line:\n",
    "        output.append(get_word_repr(letter_int, letter))\n",
    "    return np.array(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert from one hot to letter\n",
    "def vect_to_letter(vect, int_letter):\n",
    "    max_index = 0\n",
    "    max_value = 0\n",
    "    for curr_index, curr_value in enumerate(vect):\n",
    "        if max_value < curr_value:\n",
    "            max_index = curr_index\n",
    "            max_value = curr_value\n",
    "    return int_letter[max_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a RNN generates a shakespeare sonnet\n",
    "def generate_poem(RNN, letter_int, int_letter):\n",
    "    # Initial line to seed \n",
    "    first_line = \"shall i compare thee to a summer\\'s day?\\n\"\n",
    "    \n",
    "    curr_line = convert_line_to_one_hot(first_line, letter_int)\n",
    "    curr_poem = first_line\n",
    "    line_num = 2\n",
    "    while line_num < 15:\n",
    "        # Predict the next character\n",
    "        next_char = RNN.predict(np.array([curr_line]))[0]\n",
    "        # Update curr_line and curr_poem\n",
    "        curr_poem += vect_to_letter(next_char, int_letter)\n",
    "        if vect_to_letter(next_char, int_letter) == \"\\n\":\n",
    "            line_num += 1\n",
    "        curr_line = convert_line_to_one_hot(curr_poem[-40:], letter_int)\n",
    "    \n",
    "    return curr_poem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poem = generate_poem(model, letter_int, int_letter)\n",
    "print(poem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
