{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Some of the methods for this problem were reused from homework 5\n",
    "\n",
    "import pickle\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Activation, Lambda\n",
    "import numpy as np\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "letter_data = pickle.load(open(\"data/letter_data.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionaries to convert between letter and index\n",
    "letter_int = {}\n",
    "int_letter = {}\n",
    "i = 0\n",
    "for poem in letter_data:\n",
    "    for letter in poem:\n",
    "        if letter not in letter_int:\n",
    "            letter_int[letter] = i\n",
    "            int_letter[i] = letter\n",
    "            i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns one-hot-encoded feature representation of the specified word given\n",
    "# a dictionary mapping words to their one-hot-encoded index.\n",
    "def get_word_repr(letter_to_int, word):\n",
    "    unique_words = letter_to_int.keys()\n",
    "    # Return a vector that's zero everywhere besides the index corresponding to <word>\n",
    "    feature_representation = np.zeros(len(unique_words))\n",
    "    feature_representation[letter_to_int[word]] = 1\n",
    "    return feature_representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_traindata(word_list, word_to_index, window_size=40, skip = 7):\n",
    "    \"\"\"\n",
    "    Generates training data for Skipgram model (sort of).\n",
    "\n",
    "    Arguments:\n",
    "        word_list:     Sequential list of letters (strings).\n",
    "        word_to_index: Dictionary mapping words to their corresponding index\n",
    "                       in a one-hot-encoded representation of our corpus.\n",
    "\n",
    "        window_size:   Size of Skipgram window.\n",
    "        \n",
    "        skip:          Skip every skip characters \n",
    "\n",
    "    Returns:\n",
    "        (trainX, trainY):     A pair of matrices (trainX, trainY) containing training\n",
    "                              points (one-hot-encoded vectors representing individual words) and\n",
    "                              their corresponding labels (also one-hot-encoded vectors representing words).\n",
    "    \"\"\"\n",
    "    trainX = []\n",
    "    trainY = []\n",
    "    for i in range(window_size, len(word_list), skip):\n",
    "        curr_word = word_list[i]\n",
    "        curr_X = []\n",
    "        for j in range(-window_size, 0):\n",
    "            if j != 0 and i + j >= 0 and i + j < len(word_list):\n",
    "                adjacent_word = word_list[i + j]\n",
    "                curr_X.append(get_word_repr(word_to_index, adjacent_word))\n",
    "        trainX.append(curr_X)\n",
    "        trainY.append(get_word_repr(word_to_index, curr_word))\n",
    "        \n",
    "    return (np.array(trainX), np.array(trainY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the training set\n",
    "unit = True\n",
    "train_x = -1\n",
    "train_y = -1\n",
    "for poem in letter_data:\n",
    "    poem_train_x, poem_train_y = generate_traindata(poem, letter_int)\n",
    "    if unit:\n",
    "        train_x = poem_train_x\n",
    "        train_y = poem_train_y\n",
    "        unit = False\n",
    "    else:\n",
    "        train_x = np.concatenate((train_x, poem_train_x))\n",
    "        train_y = np.concatenate((train_y, poem_train_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 180)               157680    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 38)                6878      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 38)                0         \n",
      "_________________________________________________________________\n",
      "lambda_1 (Lambda)            (None, 38)                0         \n",
      "=================================================================\n",
      "Total params: 164,558\n",
      "Trainable params: 164,558\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/75\n",
      "12576/12576 [==============================] - 11s 905us/step - loss: 3.0089 - acc: 0.1652\n",
      "Epoch 2/75\n",
      "12576/12576 [==============================] - 11s 854us/step - loss: 2.8309 - acc: 0.2129\n",
      "Epoch 3/75\n",
      "12576/12576 [==============================] - 11s 900us/step - loss: 2.5977 - acc: 0.2794\n",
      "Epoch 4/75\n",
      "12576/12576 [==============================] - 12s 945us/step - loss: 2.4344 - acc: 0.3146\n",
      "Epoch 5/75\n",
      "12576/12576 [==============================] - 12s 921us/step - loss: 2.3386 - acc: 0.3310\n",
      "Epoch 6/75\n",
      "12576/12576 [==============================] - 11s 895us/step - loss: 2.2717 - acc: 0.3450\n",
      "Epoch 7/75\n",
      "12576/12576 [==============================] - 11s 895us/step - loss: 2.2183 - acc: 0.3542\n",
      "Epoch 8/75\n",
      "12576/12576 [==============================] - 10s 825us/step - loss: 2.1747 - acc: 0.3653\n",
      "Epoch 9/75\n",
      "12576/12576 [==============================] - 10s 816us/step - loss: 2.1316 - acc: 0.3754\n",
      "Epoch 10/75\n",
      "12576/12576 [==============================] - 10s 826us/step - loss: 2.0927 - acc: 0.3844\n",
      "Epoch 11/75\n",
      "12576/12576 [==============================] - 11s 880us/step - loss: 2.0560 - acc: 0.3895\n",
      "Epoch 12/75\n",
      "12576/12576 [==============================] - 12s 920us/step - loss: 2.0225 - acc: 0.4004\n",
      "Epoch 13/75\n",
      "12576/12576 [==============================] - 12s 951us/step - loss: 1.9884 - acc: 0.4062\n",
      "Epoch 14/75\n",
      "12576/12576 [==============================] - 13s 1ms/step - loss: 1.9550 - acc: 0.4109\n",
      "Epoch 15/75\n",
      "12576/12576 [==============================] - 13s 1ms/step - loss: 1.9242 - acc: 0.4202\n",
      "Epoch 16/75\n",
      "12576/12576 [==============================] - 12s 989us/step - loss: 1.8873 - acc: 0.4338\n",
      "Epoch 17/75\n",
      "12576/12576 [==============================] - 12s 988us/step - loss: 1.8586 - acc: 0.4392\n",
      "Epoch 18/75\n",
      "12576/12576 [==============================] - 12s 951us/step - loss: 1.8194 - acc: 0.4513\n",
      "Epoch 19/75\n",
      "12576/12576 [==============================] - 11s 914us/step - loss: 1.7859 - acc: 0.4623\n",
      "Epoch 20/75\n",
      "12576/12576 [==============================] - 12s 978us/step - loss: 1.7506 - acc: 0.4681\n",
      "Epoch 21/75\n",
      "12576/12576 [==============================] - 12s 962us/step - loss: 1.7097 - acc: 0.4836\n",
      "Epoch 22/75\n",
      "12576/12576 [==============================] - 11s 899us/step - loss: 1.6716 - acc: 0.4913\n",
      "Epoch 23/75\n",
      "12576/12576 [==============================] - 11s 893us/step - loss: 1.6313 - acc: 0.5053\n",
      "Epoch 24/75\n",
      "12576/12576 [==============================] - 12s 972us/step - loss: 1.5908 - acc: 0.5148\n",
      "Epoch 25/75\n",
      "12576/12576 [==============================] - 12s 970us/step - loss: 1.5430 - acc: 0.5329\n",
      "Epoch 26/75\n",
      "12576/12576 [==============================] - 13s 1ms/step - loss: 1.4971 - acc: 0.5445\n",
      "Epoch 27/75\n",
      "12576/12576 [==============================] - 12s 945us/step - loss: 1.4434 - acc: 0.5627\n",
      "Epoch 28/75\n",
      "12576/12576 [==============================] - 11s 903us/step - loss: 1.3956 - acc: 0.5785\n",
      "Epoch 29/75\n",
      "12576/12576 [==============================] - 12s 957us/step - loss: 1.3386 - acc: 0.5955\n",
      "Epoch 30/75\n",
      "12576/12576 [==============================] - 12s 921us/step - loss: 1.2860 - acc: 0.6151\n",
      "Epoch 31/75\n",
      "12576/12576 [==============================] - 11s 900us/step - loss: 1.2238 - acc: 0.6346\n",
      "Epoch 32/75\n",
      "12576/12576 [==============================] - 11s 896us/step - loss: 1.1636 - acc: 0.6518\n",
      "Epoch 33/75\n",
      "12576/12576 [==============================] - 11s 891us/step - loss: 1.1047 - acc: 0.6745\n",
      "Epoch 34/75\n",
      "12576/12576 [==============================] - 12s 919us/step - loss: 1.0433 - acc: 0.6974\n",
      "Epoch 35/75\n",
      "12576/12576 [==============================] - 11s 904us/step - loss: 0.9871 - acc: 0.7152\n",
      "Epoch 36/75\n",
      "12576/12576 [==============================] - 11s 899us/step - loss: 0.9201 - acc: 0.7411\n",
      "Epoch 37/75\n",
      "12576/12576 [==============================] - 11s 897us/step - loss: 0.8585 - acc: 0.7634\n",
      "Epoch 38/75\n",
      "12576/12576 [==============================] - 12s 962us/step - loss: 0.8016 - acc: 0.7816\n",
      "Epoch 39/75\n",
      "12576/12576 [==============================] - 12s 949us/step - loss: 0.7423 - acc: 0.8027\n",
      "Epoch 40/75\n",
      "12576/12576 [==============================] - 12s 949us/step - loss: 0.6870 - acc: 0.8237\n",
      "Epoch 41/75\n",
      "12576/12576 [==============================] - 12s 949us/step - loss: 0.6256 - acc: 0.8399\n",
      "Epoch 42/75\n",
      "12576/12576 [==============================] - 12s 934us/step - loss: 0.5787 - acc: 0.8590\n",
      "Epoch 43/75\n",
      "12576/12576 [==============================] - 12s 937us/step - loss: 0.5305 - acc: 0.8734\n",
      "Epoch 44/75\n",
      "12576/12576 [==============================] - 12s 988us/step - loss: 0.4785 - acc: 0.8902\n",
      "Epoch 45/75\n",
      "12576/12576 [==============================] - 13s 1ms/step - loss: 0.4422 - acc: 0.9022\n",
      "Epoch 46/75\n",
      "12576/12576 [==============================] - 12s 973us/step - loss: 0.4000 - acc: 0.9144\n",
      "Epoch 47/75\n",
      "12576/12576 [==============================] - 12s 935us/step - loss: 0.3627 - acc: 0.9241\n",
      "Epoch 48/75\n",
      "12576/12576 [==============================] - 11s 882us/step - loss: 0.3264 - acc: 0.9376\n",
      "Epoch 49/75\n",
      "12576/12576 [==============================] - 12s 929us/step - loss: 0.2973 - acc: 0.9439\n",
      "Epoch 50/75\n",
      "12576/12576 [==============================] - 13s 1ms/step - loss: 0.2697 - acc: 0.9530\n",
      "Epoch 51/75\n",
      "12576/12576 [==============================] - 12s 973us/step - loss: 0.2465 - acc: 0.9573\n",
      "Epoch 52/75\n",
      "12576/12576 [==============================] - 11s 907us/step - loss: 0.2240 - acc: 0.9623\n",
      "Epoch 53/75\n",
      "12576/12576 [==============================] - 11s 907us/step - loss: 0.2032 - acc: 0.9682\n",
      "Epoch 54/75\n",
      "12576/12576 [==============================] - 12s 978us/step - loss: 0.1886 - acc: 0.9716\n",
      "Epoch 55/75\n",
      "12576/12576 [==============================] - 12s 958us/step - loss: 0.1713 - acc: 0.9768\n",
      "Epoch 56/75\n",
      "12576/12576 [==============================] - 13s 1ms/step - loss: 0.1573 - acc: 0.9786\n",
      "Epoch 57/75\n",
      "12576/12576 [==============================] - 13s 1ms/step - loss: 0.1474 - acc: 0.9785\n",
      "Epoch 58/75\n",
      "12576/12576 [==============================] - 12s 934us/step - loss: 0.1323 - acc: 0.9843\n",
      "Epoch 59/75\n",
      "12576/12576 [==============================] - 11s 897us/step - loss: 0.1244 - acc: 0.9853\n",
      "Epoch 60/75\n",
      "12576/12576 [==============================] - 11s 914us/step - loss: 0.1169 - acc: 0.9858\n",
      "Epoch 61/75\n",
      "12576/12576 [==============================] - 12s 946us/step - loss: 0.1081 - acc: 0.9881\n",
      "Epoch 62/75\n",
      "12576/12576 [==============================] - 12s 977us/step - loss: 0.1021 - acc: 0.9899\n",
      "Epoch 63/75\n",
      "12576/12576 [==============================] - 12s 919us/step - loss: 0.0969 - acc: 0.9891\n",
      "Epoch 64/75\n",
      "12576/12576 [==============================] - 12s 975us/step - loss: 0.0963 - acc: 0.9882\n",
      "Epoch 65/75\n",
      "12576/12576 [==============================] - 12s 949us/step - loss: 0.0864 - acc: 0.9901\n",
      "Epoch 66/75\n",
      "12576/12576 [==============================] - 12s 939us/step - loss: 0.0861 - acc: 0.9891\n",
      "Epoch 67/75\n",
      "12576/12576 [==============================] - 13s 996us/step - loss: 0.0828 - acc: 0.9905\n",
      "Epoch 68/75\n",
      "12576/12576 [==============================] - 12s 956us/step - loss: 0.0779 - acc: 0.9905\n",
      "Epoch 69/75\n",
      "12576/12576 [==============================] - 13s 994us/step - loss: 0.0721 - acc: 0.9920\n",
      "Epoch 70/75\n",
      "12576/12576 [==============================] - 12s 928us/step - loss: 0.0753 - acc: 0.9894\n",
      "Epoch 71/75\n",
      "12576/12576 [==============================] - 12s 954us/step - loss: 0.0676 - acc: 0.9912\n",
      "Epoch 72/75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12576/12576 [==============================] - 12s 965us/step - loss: 0.0654 - acc: 0.9917\n",
      "Epoch 73/75\n",
      "12576/12576 [==============================] - 12s 991us/step - loss: 0.0719 - acc: 0.9900\n",
      "Epoch 74/75\n",
      "12576/12576 [==============================] - 12s 942us/step - loss: 0.0590 - acc: 0.9935\n",
      "Epoch 75/75\n",
      "12576/12576 [==============================] - 11s 894us/step - loss: 0.0608 - acc: 0.9917\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fdc73894da0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate the neural network\n",
    "model = Sequential()\n",
    "model.add(LSTM(180, input_shape = (len(train_x[0]), len(train_x[0][0]))))\n",
    "model.add(Dense(len(train_y[0])))\n",
    "model.add(Activation('softmax'))\n",
    "model.add(Lambda(lambda x: x / 1.5))\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss = \"categorical_crossentropy\", optimizer = \"rmsprop\", metrics = ['accuracy'])\n",
    "model.fit(train_x, train_y, epochs = 75, batch_size = 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert a line to one hot form\n",
    "def convert_line_to_one_hot(line, letter_int):\n",
    "    output = []\n",
    "    for letter in line:\n",
    "        output.append(get_word_repr(letter_int, letter))\n",
    "    return np.array(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert from one hot to letter\n",
    "def vect_to_letter(vect, int_letter):\n",
    "    max_index = 0\n",
    "    max_value = 0\n",
    "    for curr_index, curr_value in enumerate(vect):\n",
    "        if max_value < curr_value:\n",
    "            max_index = curr_index\n",
    "            max_value = curr_value\n",
    "    return int_letter[max_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a RNN generates a shakespeare sonnet\n",
    "def generate_poem(RNN, letter_int, int_letter):\n",
    "    # Initial line to seed \n",
    "    first_line = \"shall i compare thee to a summer\\'s day?\\n\"\n",
    "    \n",
    "    curr_line = convert_line_to_one_hot(first_line, letter_int)\n",
    "    curr_poem = first_line\n",
    "    line_num = 2\n",
    "    while line_num < 15:\n",
    "        # Predict the next character\n",
    "        next_char = RNN.predict(np.array([curr_line]))[0]\n",
    "        # Update curr_line and curr_poem\n",
    "        curr_poem += vect_to_letter(next_char, int_letter)\n",
    "        if vect_to_letter(next_char, int_letter) == \"\\n\":\n",
    "            line_num += 1\n",
    "        curr_line = convert_line_to_one_hot(curr_poem[-40:], letter_int)\n",
    "    \n",
    "    return curr_poem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shall i compare thee to a summer's day?\n",
      "the fremme then menp's fille whe brackey my ming,\n",
      "since hack oo sulleds re ppornes withing in lleds,\n",
      "and hrandy theer avered what hir pore,\n",
      "whin thou wald be farton worth farring preass,\n",
      "bot am im chindoun hayey in thee part,\n",
      "fom choummyes thou stif there thing in dee,\n",
      "and is llove bititish or sulloud hiedome have,\n",
      "shat thou lats thoug and be a'th faice,\n",
      "the areast of oo his foull doun all:\n",
      "leans you drus an ille ey hatt,\n",
      "you all you mond reer thine in the weel,\n",
      "coom in therain llogg talllowhed bllodsers menjurnowndo sprobt,\n",
      "whan stolll seapt tree lovery herring gicing,\n",
      "\n"
     ]
    }
   ],
   "source": [
    "poem = generate_poem(model, letter_int, int_letter)\n",
    "print(poem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
