{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Some of the methods for this problem were reused from homework 5\n",
    "\n",
    "import pickle\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Activation\n",
    "import numpy as np\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "letter_data = pickle.load(open(\"data/letter_data.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionaries to convert between letter and index\n",
    "letter_int = {}\n",
    "int_letter = {}\n",
    "i = 0\n",
    "for poem in letter_data:\n",
    "    for letter in poem:\n",
    "        if letter not in letter_int:\n",
    "            letter_int[letter] = i\n",
    "            int_letter[i] = letter\n",
    "            i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns one-hot-encoded feature representation of the specified word given\n",
    "# a dictionary mapping words to their one-hot-encoded index.\n",
    "def get_word_repr(letter_to_int, word):\n",
    "    unique_words = letter_to_int.keys()\n",
    "    # Return a vector that's zero everywhere besides the index corresponding to <word>\n",
    "    feature_representation = np.zeros(len(unique_words))\n",
    "    feature_representation[letter_to_int[word]] = 1\n",
    "    return feature_representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_traindata(word_list, word_to_index, window_size=40, skip = 5):\n",
    "    \"\"\"\n",
    "    Generates training data for Skipgram model (sort of).\n",
    "\n",
    "    Arguments:\n",
    "        word_list:     Sequential list of letters (strings).\n",
    "        word_to_index: Dictionary mapping words to their corresponding index\n",
    "                       in a one-hot-encoded representation of our corpus.\n",
    "\n",
    "        window_size:   Size of Skipgram window.\n",
    "        \n",
    "        skip:          Skip every skip characters \n",
    "\n",
    "    Returns:\n",
    "        (trainX, trainY):     A pair of matrices (trainX, trainY) containing training\n",
    "                              points (one-hot-encoded vectors representing individual words) and\n",
    "                              their corresponding labels (also one-hot-encoded vectors representing words).\n",
    "    \"\"\"\n",
    "    trainX = []\n",
    "    trainY = []\n",
    "    for i in range(window_size, len(word_list), skip):\n",
    "        curr_word = word_list[i]\n",
    "        curr_X = []\n",
    "        for j in range(-window_size, 0):\n",
    "            if j != 0 and i + j >= 0 and i + j < len(word_list):\n",
    "                adjacent_word = word_list[i + j]\n",
    "                curr_X.append(get_word_repr(word_to_index, adjacent_word))\n",
    "        trainX.append(curr_X)\n",
    "        trainY.append(get_word_repr(word_to_index, curr_word))\n",
    "        \n",
    "    return (np.array(trainX), np.array(trainY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the training set\n",
    "unit = True\n",
    "train_x = -1\n",
    "train_y = -1\n",
    "for poem in letter_data:\n",
    "    poem_train_x, poem_train_y = generate_traindata(poem, letter_int)\n",
    "    if unit:\n",
    "        train_x = poem_train_x\n",
    "        train_y = poem_train_y\n",
    "        unit = False\n",
    "    else:\n",
    "        train_x = np.concatenate((train_x, poem_train_x))\n",
    "        train_y = np.concatenate((train_y, poem_train_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               55600     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 38)                3838      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 38)                0         \n",
      "=================================================================\n",
      "Total params: 59,438\n",
      "Trainable params: 59,438\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "17573/17573 [==============================] - 8s 481us/step - loss: 3.0030 - acc: 0.1627\n",
      "Epoch 2/200\n",
      "17573/17573 [==============================] - 8s 454us/step - loss: 2.8252 - acc: 0.2086\n",
      "Epoch 3/200\n",
      "17573/17573 [==============================] - 8s 467us/step - loss: 2.5915 - acc: 0.2822\n",
      "Epoch 4/200\n",
      "17573/17573 [==============================] - 8s 456us/step - loss: 2.4263 - acc: 0.3125\n",
      "Epoch 5/200\n",
      "17573/17573 [==============================] - 8s 461us/step - loss: 2.3273 - acc: 0.3289\n",
      "Epoch 6/200\n",
      "17573/17573 [==============================] - 8s 459us/step - loss: 2.2600 - acc: 0.3474\n",
      "Epoch 7/200\n",
      "17573/17573 [==============================] - 8s 459us/step - loss: 2.2097 - acc: 0.3588\n",
      "Epoch 8/200\n",
      "17573/17573 [==============================] - 8s 464us/step - loss: 2.1643 - acc: 0.3698\n",
      "Epoch 9/200\n",
      "17573/17573 [==============================] - 8s 467us/step - loss: 2.1258 - acc: 0.3771\n",
      "Epoch 10/200\n",
      "17573/17573 [==============================] - 8s 465us/step - loss: 2.0888 - acc: 0.3880\n",
      "Epoch 11/200\n",
      "17573/17573 [==============================] - 8s 466us/step - loss: 2.0528 - acc: 0.3946\n",
      "Epoch 12/200\n",
      "17573/17573 [==============================] - 8s 465us/step - loss: 2.0190 - acc: 0.4044\n",
      "Epoch 13/200\n",
      "17573/17573 [==============================] - 8s 469us/step - loss: 1.9865 - acc: 0.4127\n",
      "Epoch 14/200\n",
      "17573/17573 [==============================] - 8s 470us/step - loss: 1.9595 - acc: 0.4195\n",
      "Epoch 15/200\n",
      "17573/17573 [==============================] - 8s 467us/step - loss: 1.9337 - acc: 0.4264\n",
      "Epoch 16/200\n",
      "17573/17573 [==============================] - 8s 469us/step - loss: 1.9074 - acc: 0.4339\n",
      "Epoch 17/200\n",
      "17573/17573 [==============================] - 8s 470us/step - loss: 1.8847 - acc: 0.4387\n",
      "Epoch 18/200\n",
      "17573/17573 [==============================] - 8s 473us/step - loss: 1.8630 - acc: 0.4447\n",
      "Epoch 19/200\n",
      "17573/17573 [==============================] - 8s 473us/step - loss: 1.8383 - acc: 0.4510\n",
      "Epoch 20/200\n",
      "17573/17573 [==============================] - 8s 475us/step - loss: 1.8173 - acc: 0.4563\n",
      "Epoch 21/200\n",
      "17573/17573 [==============================] - 8s 479us/step - loss: 1.7948 - acc: 0.4637\n",
      "Epoch 22/200\n",
      "17573/17573 [==============================] - 8s 477us/step - loss: 1.7741 - acc: 0.4673\n",
      "Epoch 23/200\n",
      "17573/17573 [==============================] - 8s 479us/step - loss: 1.7510 - acc: 0.4732\n",
      "Epoch 24/200\n",
      "17573/17573 [==============================] - 9s 484us/step - loss: 1.7295 - acc: 0.4799\n",
      "Epoch 25/200\n",
      "17573/17573 [==============================] - 8s 479us/step - loss: 1.7077 - acc: 0.4848\n",
      "Epoch 26/200\n",
      "17573/17573 [==============================] - 8s 480us/step - loss: 1.6868 - acc: 0.4904\n",
      "Epoch 27/200\n",
      "17573/17573 [==============================] - 8s 480us/step - loss: 1.6643 - acc: 0.4984\n",
      "Epoch 28/200\n",
      "17573/17573 [==============================] - 8s 478us/step - loss: 1.6400 - acc: 0.5025\n",
      "Epoch 29/200\n",
      "17573/17573 [==============================] - 8s 479us/step - loss: 1.6163 - acc: 0.5108\n",
      "Epoch 30/200\n",
      "17573/17573 [==============================] - 8s 479us/step - loss: 1.5941 - acc: 0.5171\n",
      "Epoch 31/200\n",
      "17573/17573 [==============================] - 8s 478us/step - loss: 1.5689 - acc: 0.5243\n",
      "Epoch 32/200\n",
      "17573/17573 [==============================] - 8s 477us/step - loss: 1.5459 - acc: 0.5304\n",
      "Epoch 33/200\n",
      "17573/17573 [==============================] - 8s 479us/step - loss: 1.5228 - acc: 0.5367\n",
      "Epoch 34/200\n",
      "17573/17573 [==============================] - 8s 479us/step - loss: 1.4971 - acc: 0.5466\n",
      "Epoch 35/200\n",
      "17573/17573 [==============================] - 8s 480us/step - loss: 1.4716 - acc: 0.5535\n",
      "Epoch 36/200\n",
      "17573/17573 [==============================] - 8s 478us/step - loss: 1.4444 - acc: 0.5646\n",
      "Epoch 37/200\n",
      "17573/17573 [==============================] - 8s 479us/step - loss: 1.4194 - acc: 0.5700\n",
      "Epoch 38/200\n",
      "17573/17573 [==============================] - 8s 482us/step - loss: 1.3952 - acc: 0.5752\n",
      "Epoch 39/200\n",
      "17573/17573 [==============================] - 8s 481us/step - loss: 1.3689 - acc: 0.5817\n",
      "Epoch 40/200\n",
      "17573/17573 [==============================] - 8s 484us/step - loss: 1.3439 - acc: 0.5918\n",
      "Epoch 41/200\n",
      "17573/17573 [==============================] - 8s 482us/step - loss: 1.3195 - acc: 0.6035\n",
      "Epoch 42/200\n",
      "17573/17573 [==============================] - 8s 481us/step - loss: 1.2903 - acc: 0.6075\n",
      "Epoch 43/200\n",
      "17573/17573 [==============================] - 8s 481us/step - loss: 1.2681 - acc: 0.6159\n",
      "Epoch 44/200\n",
      "17573/17573 [==============================] - 8s 480us/step - loss: 1.2402 - acc: 0.6224\n",
      "Epoch 45/200\n",
      "17573/17573 [==============================] - 8s 482us/step - loss: 1.2142 - acc: 0.6340\n",
      "Epoch 46/200\n",
      "17573/17573 [==============================] - 9s 488us/step - loss: 1.1881 - acc: 0.6425\n",
      "Epoch 47/200\n",
      "17573/17573 [==============================] - 8s 467us/step - loss: 1.1634 - acc: 0.6487\n",
      "Epoch 48/200\n",
      "17573/17573 [==============================] - 8s 459us/step - loss: 1.1369 - acc: 0.6589\n",
      "Epoch 49/200\n",
      "17573/17573 [==============================] - 8s 459us/step - loss: 1.1084 - acc: 0.6687\n",
      "Epoch 50/200\n",
      "17573/17573 [==============================] - 8s 460us/step - loss: 1.0834 - acc: 0.6761\n",
      "Epoch 51/200\n",
      "17573/17573 [==============================] - 8s 456us/step - loss: 1.0588 - acc: 0.6810\n",
      "Epoch 52/200\n",
      "17573/17573 [==============================] - 8s 461us/step - loss: 1.0360 - acc: 0.6870\n",
      "Epoch 53/200\n",
      "17573/17573 [==============================] - 8s 462us/step - loss: 1.0083 - acc: 0.7005\n",
      "Epoch 54/200\n",
      "17573/17573 [==============================] - 8s 460us/step - loss: 0.9819 - acc: 0.7093\n",
      "Epoch 55/200\n",
      "17573/17573 [==============================] - 8s 458us/step - loss: 0.9588 - acc: 0.7158\n",
      "Epoch 56/200\n",
      "17573/17573 [==============================] - 8s 456us/step - loss: 0.9331 - acc: 0.7217\n",
      "Epoch 57/200\n",
      "17573/17573 [==============================] - 8s 476us/step - loss: 0.9115 - acc: 0.7312\n",
      "Epoch 58/200\n",
      "17573/17573 [==============================] - 8s 466us/step - loss: 0.8854 - acc: 0.7381\n",
      "Epoch 59/200\n",
      "17573/17573 [==============================] - 8s 463us/step - loss: 0.8676 - acc: 0.7445\n",
      "Epoch 60/200\n",
      "17573/17573 [==============================] - 8s 457us/step - loss: 0.8423 - acc: 0.7552\n",
      "Epoch 61/200\n",
      "17573/17573 [==============================] - 8s 458us/step - loss: 0.8186 - acc: 0.7610\n",
      "Epoch 62/200\n",
      "17573/17573 [==============================] - 8s 458us/step - loss: 0.7994 - acc: 0.7687\n",
      "Epoch 63/200\n",
      "17573/17573 [==============================] - 8s 462us/step - loss: 0.7804 - acc: 0.7758\n",
      "Epoch 64/200\n",
      "17573/17573 [==============================] - 8s 460us/step - loss: 0.7603 - acc: 0.7829\n",
      "Epoch 65/200\n",
      "17573/17573 [==============================] - 8s 458us/step - loss: 0.7385 - acc: 0.7889\n",
      "Epoch 66/200\n",
      "17573/17573 [==============================] - 8s 457us/step - loss: 0.7208 - acc: 0.7942\n",
      "Epoch 67/200\n",
      "17573/17573 [==============================] - 8s 461us/step - loss: 0.7032 - acc: 0.8005\n",
      "Epoch 68/200\n",
      "17573/17573 [==============================] - 8s 460us/step - loss: 0.6807 - acc: 0.8093\n",
      "Epoch 69/200\n",
      "17573/17573 [==============================] - 8s 460us/step - loss: 0.6627 - acc: 0.8142\n",
      "Epoch 70/200\n",
      "17573/17573 [==============================] - 8s 462us/step - loss: 0.6467 - acc: 0.8198\n",
      "Epoch 71/200\n",
      "17573/17573 [==============================] - 8s 461us/step - loss: 0.6315 - acc: 0.8242\n",
      "Epoch 72/200\n",
      "17573/17573 [==============================] - 8s 462us/step - loss: 0.6133 - acc: 0.8315\n",
      "Epoch 73/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17573/17573 [==============================] - 8s 441us/step - loss: 0.5984 - acc: 0.8351\n",
      "Epoch 74/200\n",
      "17573/17573 [==============================] - 8s 440us/step - loss: 0.5847 - acc: 0.8374\n",
      "Epoch 75/200\n",
      "17573/17573 [==============================] - 8s 442us/step - loss: 0.5667 - acc: 0.8472\n",
      "Epoch 76/200\n",
      "17573/17573 [==============================] - 8s 442us/step - loss: 0.5525 - acc: 0.8524\n",
      "Epoch 77/200\n",
      "17573/17573 [==============================] - 8s 440us/step - loss: 0.5428 - acc: 0.8515\n",
      "Epoch 78/200\n",
      "17573/17573 [==============================] - 8s 445us/step - loss: 0.5263 - acc: 0.8585\n",
      "Epoch 79/200\n",
      "17573/17573 [==============================] - 8s 469us/step - loss: 0.5154 - acc: 0.8599\n",
      "Epoch 80/200\n",
      "17573/17573 [==============================] - 9s 502us/step - loss: 0.5057 - acc: 0.8617\n",
      "Epoch 81/200\n",
      "17573/17573 [==============================] - 8s 483us/step - loss: 0.4884 - acc: 0.8700\n",
      "Epoch 82/200\n",
      "17573/17573 [==============================] - 8s 479us/step - loss: 0.4795 - acc: 0.8719\n",
      "Epoch 83/200\n",
      "17573/17573 [==============================] - 8s 462us/step - loss: 0.4653 - acc: 0.8771\n",
      "Epoch 84/200\n",
      "17573/17573 [==============================] - 8s 461us/step - loss: 0.4594 - acc: 0.8771\n",
      "Epoch 85/200\n",
      "17573/17573 [==============================] - 8s 463us/step - loss: 0.4442 - acc: 0.8820\n",
      "Epoch 86/200\n",
      "17573/17573 [==============================] - 8s 455us/step - loss: 0.4375 - acc: 0.8850\n",
      "Epoch 87/200\n",
      "17573/17573 [==============================] - 8s 455us/step - loss: 0.4254 - acc: 0.8876\n",
      "Epoch 88/200\n",
      "17573/17573 [==============================] - 8s 456us/step - loss: 0.4139 - acc: 0.8921\n",
      "Epoch 89/200\n",
      "17573/17573 [==============================] - 8s 451us/step - loss: 0.4060 - acc: 0.8952\n",
      "Epoch 90/200\n",
      "17573/17573 [==============================] - 8s 454us/step - loss: 0.3988 - acc: 0.8969\n",
      "Epoch 91/200\n",
      "17573/17573 [==============================] - 8s 453us/step - loss: 0.3860 - acc: 0.9030\n",
      "Epoch 92/200\n",
      "17573/17573 [==============================] - 8s 458us/step - loss: 0.3789 - acc: 0.9043\n",
      "Epoch 93/200\n",
      "17573/17573 [==============================] - 8s 459us/step - loss: 0.3718 - acc: 0.9025\n",
      "Epoch 94/200\n",
      "17573/17573 [==============================] - 8s 459us/step - loss: 0.3588 - acc: 0.9108\n",
      "Epoch 95/200\n",
      "17573/17573 [==============================] - 8s 452us/step - loss: 0.3601 - acc: 0.9064\n",
      "Epoch 96/200\n",
      "17573/17573 [==============================] - 8s 452us/step - loss: 0.3441 - acc: 0.9145\n",
      "Epoch 97/200\n",
      "17573/17573 [==============================] - 8s 455us/step - loss: 0.3398 - acc: 0.9146\n",
      "Epoch 98/200\n",
      "17573/17573 [==============================] - 8s 453us/step - loss: 0.3309 - acc: 0.9194\n",
      "Epoch 99/200\n",
      "17573/17573 [==============================] - 8s 452us/step - loss: 0.3316 - acc: 0.9185\n",
      "Epoch 100/200\n",
      "17573/17573 [==============================] - 8s 454us/step - loss: 0.3186 - acc: 0.9223\n",
      "Epoch 101/200\n",
      "17573/17573 [==============================] - 8s 453us/step - loss: 0.3101 - acc: 0.9233\n",
      "Epoch 102/200\n",
      "17573/17573 [==============================] - 8s 453us/step - loss: 0.3085 - acc: 0.9249\n",
      "Epoch 103/200\n",
      "17573/17573 [==============================] - 8s 453us/step - loss: 0.3074 - acc: 0.9238\n",
      "Epoch 104/200\n",
      "17573/17573 [==============================] - 8s 452us/step - loss: 0.2982 - acc: 0.9268\n",
      "Epoch 105/200\n",
      "17573/17573 [==============================] - 8s 455us/step - loss: 0.2915 - acc: 0.9313\n",
      "Epoch 106/200\n",
      "17573/17573 [==============================] - 8s 453us/step - loss: 0.2864 - acc: 0.9306\n",
      "Epoch 107/200\n",
      "17573/17573 [==============================] - 8s 451us/step - loss: 0.2838 - acc: 0.9319\n",
      "Epoch 108/200\n",
      "17573/17573 [==============================] - 8s 452us/step - loss: 0.2752 - acc: 0.9342\n",
      "Epoch 109/200\n",
      "17573/17573 [==============================] - 8s 453us/step - loss: 0.2711 - acc: 0.9346\n",
      "Epoch 110/200\n",
      "17573/17573 [==============================] - 8s 453us/step - loss: 0.2652 - acc: 0.9369\n",
      "Epoch 111/200\n",
      "17573/17573 [==============================] - 8s 455us/step - loss: 0.2616 - acc: 0.9375\n",
      "Epoch 112/200\n",
      "17573/17573 [==============================] - 9s 494us/step - loss: 0.2584 - acc: 0.9371\n",
      "Epoch 113/200\n",
      "17573/17573 [==============================] - 8s 455us/step - loss: 0.2544 - acc: 0.9407\n",
      "Epoch 114/200\n",
      "17573/17573 [==============================] - 8s 451us/step - loss: 0.2531 - acc: 0.9395\n",
      "Epoch 115/200\n",
      "17573/17573 [==============================] - 8s 453us/step - loss: 0.2472 - acc: 0.9425\n",
      "Epoch 116/200\n",
      "17573/17573 [==============================] - 8s 452us/step - loss: 0.2410 - acc: 0.9441\n",
      "Epoch 117/200\n",
      "17573/17573 [==============================] - 8s 455us/step - loss: 0.2371 - acc: 0.9425\n",
      "Epoch 118/200\n",
      "17573/17573 [==============================] - 8s 453us/step - loss: 0.2380 - acc: 0.9428\n",
      "Epoch 119/200\n",
      "17573/17573 [==============================] - 8s 469us/step - loss: 0.2270 - acc: 0.9448\n",
      "Epoch 120/200\n",
      "17573/17573 [==============================] - 8s 450us/step - loss: 0.2236 - acc: 0.9484\n",
      "Epoch 121/200\n",
      "17573/17573 [==============================] - 8s 452us/step - loss: 0.2257 - acc: 0.9464\n",
      "Epoch 122/200\n",
      "17573/17573 [==============================] - 8s 457us/step - loss: 0.2236 - acc: 0.9462\n",
      "Epoch 123/200\n",
      "17573/17573 [==============================] - 8s 457us/step - loss: 0.2183 - acc: 0.9511\n",
      "Epoch 124/200\n",
      "17573/17573 [==============================] - 8s 453us/step - loss: 0.2116 - acc: 0.9529\n",
      "Epoch 125/200\n",
      "17573/17573 [==============================] - 8s 452us/step - loss: 0.2175 - acc: 0.9473\n",
      "Epoch 126/200\n",
      "17573/17573 [==============================] - 8s 455us/step - loss: 0.2065 - acc: 0.9534\n",
      "Epoch 127/200\n",
      "17573/17573 [==============================] - 8s 457us/step - loss: 0.2086 - acc: 0.9520\n",
      "Epoch 128/200\n",
      "17573/17573 [==============================] - 8s 454us/step - loss: 0.2088 - acc: 0.9508\n",
      "Epoch 129/200\n",
      "17573/17573 [==============================] - 8s 457us/step - loss: 0.2059 - acc: 0.9512\n",
      "Epoch 130/200\n",
      "17573/17573 [==============================] - 8s 456us/step - loss: 0.1973 - acc: 0.9554\n",
      "Epoch 131/200\n",
      "17573/17573 [==============================] - 8s 459us/step - loss: 0.2043 - acc: 0.9523\n",
      "Epoch 132/200\n",
      "17573/17573 [==============================] - 8s 454us/step - loss: 0.1937 - acc: 0.9565\n",
      "Epoch 133/200\n",
      "17573/17573 [==============================] - 8s 456us/step - loss: 0.1904 - acc: 0.9566\n",
      "Epoch 134/200\n",
      "17573/17573 [==============================] - 8s 456us/step - loss: 0.1866 - acc: 0.9575\n",
      "Epoch 135/200\n",
      "17573/17573 [==============================] - 8s 453us/step - loss: 0.1835 - acc: 0.9581\n",
      "Epoch 136/200\n",
      "17573/17573 [==============================] - 8s 453us/step - loss: 0.1893 - acc: 0.9567\n",
      "Epoch 137/200\n",
      "17573/17573 [==============================] - 8s 457us/step - loss: 0.1873 - acc: 0.9561\n",
      "Epoch 138/200\n",
      "17573/17573 [==============================] - 8s 459us/step - loss: 0.1808 - acc: 0.9586\n",
      "Epoch 139/200\n",
      "17573/17573 [==============================] - 8s 455us/step - loss: 0.1789 - acc: 0.9585\n",
      "Epoch 140/200\n",
      "17573/17573 [==============================] - 8s 454us/step - loss: 0.1825 - acc: 0.9575\n",
      "Epoch 141/200\n",
      "17573/17573 [==============================] - 8s 455us/step - loss: 0.1833 - acc: 0.9579\n",
      "Epoch 142/200\n",
      "17573/17573 [==============================] - 8s 459us/step - loss: 0.1745 - acc: 0.9588\n",
      "Epoch 143/200\n",
      "17573/17573 [==============================] - 8s 455us/step - loss: 0.1687 - acc: 0.9641\n",
      "Epoch 144/200\n",
      "17573/17573 [==============================] - 8s 454us/step - loss: 0.1735 - acc: 0.9596\n",
      "Epoch 145/200\n",
      "17573/17573 [==============================] - 8s 458us/step - loss: 0.1663 - acc: 0.9627\n",
      "Epoch 146/200\n",
      "17573/17573 [==============================] - 8s 453us/step - loss: 0.1626 - acc: 0.9649\n",
      "Epoch 147/200\n",
      "17573/17573 [==============================] - 8s 455us/step - loss: 0.1707 - acc: 0.9605\n",
      "Epoch 148/200\n",
      "17573/17573 [==============================] - 8s 452us/step - loss: 0.1674 - acc: 0.9611\n",
      "Epoch 149/200\n",
      "17573/17573 [==============================] - 8s 453us/step - loss: 0.1591 - acc: 0.9646\n",
      "Epoch 150/200\n",
      "17573/17573 [==============================] - 8s 456us/step - loss: 0.1570 - acc: 0.9651\n",
      "Epoch 151/200\n",
      "17573/17573 [==============================] - 8s 455us/step - loss: 0.1630 - acc: 0.9614\n",
      "Epoch 152/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17573/17573 [==============================] - 8s 437us/step - loss: 0.1645 - acc: 0.9622\n",
      "Epoch 153/200\n",
      "17573/17573 [==============================] - 8s 436us/step - loss: 0.1572 - acc: 0.9631\n",
      "Epoch 154/200\n",
      "17573/17573 [==============================] - 8s 435us/step - loss: 0.1627 - acc: 0.9620\n",
      "Epoch 155/200\n",
      "17573/17573 [==============================] - 8s 435us/step - loss: 0.1524 - acc: 0.9657\n",
      "Epoch 156/200\n",
      "17573/17573 [==============================] - 8s 436us/step - loss: 0.1567 - acc: 0.9647\n",
      "Epoch 157/200\n",
      "17573/17573 [==============================] - 8s 436us/step - loss: 0.1504 - acc: 0.9653\n",
      "Epoch 158/200\n",
      "17573/17573 [==============================] - 8s 436us/step - loss: 0.1508 - acc: 0.9670\n",
      "Epoch 159/200\n",
      "17573/17573 [==============================] - 8s 437us/step - loss: 0.1497 - acc: 0.9660\n",
      "Epoch 160/200\n",
      "17573/17573 [==============================] - 8s 438us/step - loss: 0.1445 - acc: 0.9682\n",
      "Epoch 161/200\n",
      "17573/17573 [==============================] - 8s 435us/step - loss: 0.1445 - acc: 0.9684\n",
      "Epoch 162/200\n",
      "17573/17573 [==============================] - 8s 434us/step - loss: 0.1443 - acc: 0.9678\n",
      "Epoch 163/200\n",
      "17573/17573 [==============================] - 8s 435us/step - loss: 0.1477 - acc: 0.9651\n",
      "Epoch 164/200\n",
      "17573/17573 [==============================] - 8s 436us/step - loss: 0.1408 - acc: 0.9685\n",
      "Epoch 165/200\n",
      "17573/17573 [==============================] - 8s 437us/step - loss: 0.1443 - acc: 0.9673\n",
      "Epoch 166/200\n",
      "17573/17573 [==============================] - 8s 436us/step - loss: 0.1392 - acc: 0.9690\n",
      "Epoch 167/200\n",
      "17573/17573 [==============================] - 8s 437us/step - loss: 0.1368 - acc: 0.9693\n",
      "Epoch 168/200\n",
      "17573/17573 [==============================] - 8s 449us/step - loss: 0.1368 - acc: 0.9699\n",
      "Epoch 169/200\n",
      "17573/17573 [==============================] - 8s 434us/step - loss: 0.1390 - acc: 0.9675\n",
      "Epoch 170/200\n",
      "17573/17573 [==============================] - 8s 434us/step - loss: 0.1422 - acc: 0.9673\n",
      "Epoch 171/200\n",
      "17573/17573 [==============================] - 8s 434us/step - loss: 0.1377 - acc: 0.9683\n",
      "Epoch 172/200\n",
      "17573/17573 [==============================] - 8s 437us/step - loss: 0.1330 - acc: 0.9699\n",
      "Epoch 173/200\n",
      "17573/17573 [==============================] - 8s 436us/step - loss: 0.1368 - acc: 0.9676\n",
      "Epoch 174/200\n",
      "17573/17573 [==============================] - 8s 435us/step - loss: 0.1294 - acc: 0.9708\n",
      "Epoch 175/200\n",
      "17573/17573 [==============================] - 8s 433us/step - loss: 0.1382 - acc: 0.9686\n",
      "Epoch 176/200\n",
      "17573/17573 [==============================] - 8s 437us/step - loss: 0.1299 - acc: 0.9700\n",
      "Epoch 177/200\n",
      "17573/17573 [==============================] - 8s 434us/step - loss: 0.1375 - acc: 0.9684\n",
      "Epoch 178/200\n",
      "17573/17573 [==============================] - 8s 432us/step - loss: 0.1280 - acc: 0.9712\n",
      "Epoch 179/200\n",
      "17573/17573 [==============================] - 8s 432us/step - loss: 0.1333 - acc: 0.9691\n",
      "Epoch 180/200\n",
      "17573/17573 [==============================] - 8s 436us/step - loss: 0.1277 - acc: 0.9701\n",
      "Epoch 181/200\n",
      "17573/17573 [==============================] - 8s 432us/step - loss: 0.1249 - acc: 0.9727\n",
      "Epoch 182/200\n",
      "17573/17573 [==============================] - 8s 436us/step - loss: 0.1371 - acc: 0.9656\n",
      "Epoch 183/200\n",
      "17573/17573 [==============================] - 8s 446us/step - loss: 0.1286 - acc: 0.9702\n",
      "Epoch 184/200\n",
      "17573/17573 [==============================] - 8s 441us/step - loss: 0.1284 - acc: 0.9714\n",
      "Epoch 185/200\n",
      "17573/17573 [==============================] - 8s 434us/step - loss: 0.1305 - acc: 0.9684\n",
      "Epoch 186/200\n",
      "17573/17573 [==============================] - 8s 435us/step - loss: 0.1268 - acc: 0.9713\n",
      "Epoch 187/200\n",
      "17573/17573 [==============================] - 8s 438us/step - loss: 0.1228 - acc: 0.9708\n",
      "Epoch 188/200\n",
      "17573/17573 [==============================] - 8s 436us/step - loss: 0.1253 - acc: 0.9720\n",
      "Epoch 189/200\n",
      "17573/17573 [==============================] - 8s 443us/step - loss: 0.1233 - acc: 0.9715\n",
      "Epoch 190/200\n",
      "17573/17573 [==============================] - 8s 436us/step - loss: 0.1200 - acc: 0.9733\n",
      "Epoch 191/200\n",
      "17573/17573 [==============================] - 8s 439us/step - loss: 0.1234 - acc: 0.9725\n",
      "Epoch 192/200\n",
      "17573/17573 [==============================] - 8s 437us/step - loss: 0.1250 - acc: 0.9704\n",
      "Epoch 193/200\n",
      "17573/17573 [==============================] - 8s 435us/step - loss: 0.1151 - acc: 0.9737\n",
      "Epoch 194/200\n",
      "17573/17573 [==============================] - 8s 436us/step - loss: 0.1239 - acc: 0.9718\n",
      "Epoch 195/200\n",
      "17573/17573 [==============================] - 8s 437us/step - loss: 0.1227 - acc: 0.9716\n",
      "Epoch 196/200\n",
      "17573/17573 [==============================] - 8s 434us/step - loss: 0.1193 - acc: 0.9722\n",
      "Epoch 197/200\n",
      "17573/17573 [==============================] - 8s 436us/step - loss: 0.1161 - acc: 0.9730\n",
      "Epoch 198/200\n",
      "17573/17573 [==============================] - 8s 436us/step - loss: 0.1177 - acc: 0.9723\n",
      "Epoch 199/200\n",
      "17573/17573 [==============================] - 8s 438us/step - loss: 0.1171 - acc: 0.9734\n",
      "Epoch 200/200\n",
      "17573/17573 [==============================] - 8s 434us/step - loss: 0.1146 - acc: 0.9735\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f46ff45b780>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate the neural network\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, input_shape = (len(train_x[0]), len(train_x[0][0]))))\n",
    "model.add(Dense(len(train_y[0])))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss = \"categorical_crossentropy\", optimizer = \"rmsprop\", metrics = ['accuracy'])\n",
    "model.fit(train_x, train_y, epochs = 200, batch_size = 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "17573/17573 [==============================] - 8s 449us/step - loss: 0.1128 - acc: 0.9738\n",
      "Epoch 2/200\n",
      "17573/17573 [==============================] - 8s 454us/step - loss: 0.1152 - acc: 0.9724\n",
      "Epoch 3/200\n",
      "17573/17573 [==============================] - 8s 451us/step - loss: 0.1171 - acc: 0.9717\n",
      "Epoch 4/200\n",
      "17573/17573 [==============================] - 8s 450us/step - loss: 0.1161 - acc: 0.9720\n",
      "Epoch 5/200\n",
      "17573/17573 [==============================] - 8s 451us/step - loss: 0.1105 - acc: 0.9758\n",
      "Epoch 6/200\n",
      "17573/17573 [==============================] - 8s 457us/step - loss: 0.1123 - acc: 0.9736\n",
      "Epoch 7/200\n",
      "17573/17573 [==============================] - 8s 454us/step - loss: 0.1118 - acc: 0.9752\n",
      "Epoch 8/200\n",
      "17573/17573 [==============================] - 8s 456us/step - loss: 0.1113 - acc: 0.9745\n",
      "Epoch 9/200\n",
      "17573/17573 [==============================] - 8s 461us/step - loss: 0.1134 - acc: 0.9738\n",
      "Epoch 10/200\n",
      "17573/17573 [==============================] - 8s 457us/step - loss: 0.1075 - acc: 0.9749\n",
      "Epoch 11/200\n",
      "17573/17573 [==============================] - 8s 461us/step - loss: 0.1173 - acc: 0.9730\n",
      "Epoch 12/200\n",
      "17573/17573 [==============================] - 8s 461us/step - loss: 0.1129 - acc: 0.9735\n",
      "Epoch 13/200\n",
      "17573/17573 [==============================] - 8s 465us/step - loss: 0.1101 - acc: 0.9733\n",
      "Epoch 14/200\n",
      "17573/17573 [==============================] - 8s 466us/step - loss: 0.1073 - acc: 0.9743\n",
      "Epoch 15/200\n",
      "17573/17573 [==============================] - 8s 462us/step - loss: 0.1105 - acc: 0.9742\n",
      "Epoch 16/200\n",
      "17573/17573 [==============================] - 8s 464us/step - loss: 0.1016 - acc: 0.9775\n",
      "Epoch 17/200\n",
      "17573/17573 [==============================] - 8s 468us/step - loss: 0.1183 - acc: 0.9700\n",
      "Epoch 18/200\n",
      "17573/17573 [==============================] - 8s 462us/step - loss: 0.1138 - acc: 0.9739\n",
      "Epoch 19/200\n",
      "17573/17573 [==============================] - 8s 464us/step - loss: 0.1100 - acc: 0.9740\n",
      "Epoch 20/200\n",
      "17573/17573 [==============================] - 8s 467us/step - loss: 0.1075 - acc: 0.9750\n",
      "Epoch 21/200\n",
      "17573/17573 [==============================] - 8s 463us/step - loss: 0.1063 - acc: 0.9760\n",
      "Epoch 22/200\n",
      "17573/17573 [==============================] - 8s 464us/step - loss: 0.1051 - acc: 0.9753\n",
      "Epoch 23/200\n",
      "17573/17573 [==============================] - 8s 464us/step - loss: 0.1034 - acc: 0.9761\n",
      "Epoch 24/200\n",
      "17573/17573 [==============================] - 8s 468us/step - loss: 0.1046 - acc: 0.9758\n",
      "Epoch 25/200\n",
      "17573/17573 [==============================] - 8s 465us/step - loss: 0.1037 - acc: 0.9764\n",
      "Epoch 26/200\n",
      "17573/17573 [==============================] - 8s 464us/step - loss: 0.1010 - acc: 0.9763\n",
      "Epoch 27/200\n",
      "17573/17573 [==============================] - 8s 465us/step - loss: 0.1036 - acc: 0.9756\n",
      "Epoch 28/200\n",
      "17573/17573 [==============================] - 8s 468us/step - loss: 0.0997 - acc: 0.9767\n",
      "Epoch 29/200\n",
      "17573/17573 [==============================] - 8s 470us/step - loss: 0.0995 - acc: 0.9758\n",
      "Epoch 30/200\n",
      "17573/17573 [==============================] - 8s 466us/step - loss: 0.1018 - acc: 0.9772\n",
      "Epoch 31/200\n",
      "17573/17573 [==============================] - 8s 468us/step - loss: 0.1018 - acc: 0.9759\n",
      "Epoch 32/200\n",
      "17573/17573 [==============================] - 8s 465us/step - loss: 0.1061 - acc: 0.9744\n",
      "Epoch 33/200\n",
      "17573/17573 [==============================] - 8s 463us/step - loss: 0.1019 - acc: 0.9765\n",
      "Epoch 34/200\n",
      "17573/17573 [==============================] - 8s 466us/step - loss: 0.1045 - acc: 0.9758\n",
      "Epoch 35/200\n",
      "17573/17573 [==============================] - 8s 472us/step - loss: 0.1014 - acc: 0.9752\n",
      "Epoch 36/200\n",
      "17573/17573 [==============================] - 8s 467us/step - loss: 0.1066 - acc: 0.9745\n",
      "Epoch 37/200\n",
      "17573/17573 [==============================] - 8s 476us/step - loss: 0.0976 - acc: 0.9775\n",
      "Epoch 38/200\n",
      "17573/17573 [==============================] - 9s 509us/step - loss: 0.1016 - acc: 0.9743\n",
      "Epoch 39/200\n",
      "17573/17573 [==============================] - 8s 473us/step - loss: 0.0993 - acc: 0.9767\n",
      "Epoch 40/200\n",
      "17573/17573 [==============================] - 8s 475us/step - loss: 0.1025 - acc: 0.9740\n",
      "Epoch 41/200\n",
      "17573/17573 [==============================] - 8s 475us/step - loss: 0.1026 - acc: 0.9739\n",
      "Epoch 42/200\n",
      "17573/17573 [==============================] - 8s 478us/step - loss: 0.1056 - acc: 0.9738\n",
      "Epoch 43/200\n",
      "17573/17573 [==============================] - 9s 486us/step - loss: 0.1013 - acc: 0.9747\n",
      "Epoch 44/200\n",
      "17573/17573 [==============================] - 9s 485us/step - loss: 0.1022 - acc: 0.9735\n",
      "Epoch 45/200\n",
      "17573/17573 [==============================] - 8s 481us/step - loss: 0.0946 - acc: 0.9792\n",
      "Epoch 46/200\n",
      "17573/17573 [==============================] - 8s 482us/step - loss: 0.0934 - acc: 0.9776\n",
      "Epoch 47/200\n",
      "17573/17573 [==============================] - 8s 482us/step - loss: 0.0973 - acc: 0.9770\n",
      "Epoch 48/200\n",
      "17573/17573 [==============================] - 8s 481us/step - loss: 0.0964 - acc: 0.9771\n",
      "Epoch 49/200\n",
      "17573/17573 [==============================] - 8s 480us/step - loss: 0.0978 - acc: 0.9773\n",
      "Epoch 50/200\n",
      "17573/17573 [==============================] - 8s 480us/step - loss: 0.0973 - acc: 0.9770\n",
      "Epoch 51/200\n",
      "17573/17573 [==============================] - 8s 481us/step - loss: 0.1020 - acc: 0.9746\n",
      "Epoch 52/200\n",
      "17573/17573 [==============================] - 8s 483us/step - loss: 0.0956 - acc: 0.9769\n",
      "Epoch 53/200\n",
      "17573/17573 [==============================] - 8s 442us/step - loss: 0.1000 - acc: 0.9741\n",
      "Epoch 54/200\n",
      "17573/17573 [==============================] - 8s 439us/step - loss: 0.0961 - acc: 0.9777\n",
      "Epoch 55/200\n",
      "17573/17573 [==============================] - 8s 438us/step - loss: 0.1016 - acc: 0.9748\n",
      "Epoch 56/200\n",
      "17573/17573 [==============================] - 8s 438us/step - loss: 0.0976 - acc: 0.9758\n",
      "Epoch 57/200\n",
      "17573/17573 [==============================] - 8s 438us/step - loss: 0.0862 - acc: 0.9804\n",
      "Epoch 58/200\n",
      "17573/17573 [==============================] - 8s 443us/step - loss: 0.0941 - acc: 0.9775\n",
      "Epoch 59/200\n",
      "17573/17573 [==============================] - 8s 437us/step - loss: 0.0845 - acc: 0.9806\n",
      "Epoch 60/200\n",
      "17573/17573 [==============================] - 8s 437us/step - loss: 0.0926 - acc: 0.9776\n",
      "Epoch 61/200\n",
      "17573/17573 [==============================] - 8s 437us/step - loss: 0.1018 - acc: 0.9754\n",
      "Epoch 62/200\n",
      "17573/17573 [==============================] - 8s 438us/step - loss: 0.0981 - acc: 0.9762\n",
      "Epoch 63/200\n",
      "17573/17573 [==============================] - 8s 435us/step - loss: 0.0872 - acc: 0.9797\n",
      "Epoch 64/200\n",
      "17573/17573 [==============================] - 8s 437us/step - loss: 0.0823 - acc: 0.9813\n",
      "Epoch 65/200\n",
      "17573/17573 [==============================] - 8s 435us/step - loss: 0.0976 - acc: 0.9759\n",
      "Epoch 66/200\n",
      "17573/17573 [==============================] - 8s 439us/step - loss: 0.0906 - acc: 0.9781\n",
      "Epoch 67/200\n",
      "17573/17573 [==============================] - 8s 437us/step - loss: 0.0906 - acc: 0.9790\n",
      "Epoch 68/200\n",
      "17573/17573 [==============================] - 8s 441us/step - loss: 0.0918 - acc: 0.9784\n",
      "Epoch 69/200\n",
      "17573/17573 [==============================] - 8s 438us/step - loss: 0.0906 - acc: 0.9774\n",
      "Epoch 70/200\n",
      "17573/17573 [==============================] - 8s 437us/step - loss: 0.0916 - acc: 0.9774\n",
      "Epoch 71/200\n",
      "17573/17573 [==============================] - 8s 438us/step - loss: 0.0931 - acc: 0.9763\n",
      "Epoch 72/200\n",
      "17573/17573 [==============================] - 8s 438us/step - loss: 0.0893 - acc: 0.9789\n",
      "Epoch 73/200\n",
      "17573/17573 [==============================] - 8s 441us/step - loss: 0.0914 - acc: 0.9775\n",
      "Epoch 74/200\n",
      "17573/17573 [==============================] - 8s 440us/step - loss: 0.0923 - acc: 0.9769\n",
      "Epoch 75/200\n",
      "17573/17573 [==============================] - 8s 442us/step - loss: 0.0850 - acc: 0.9795\n",
      "Epoch 76/200\n",
      "17573/17573 [==============================] - 9s 490us/step - loss: 0.0853 - acc: 0.9795\n",
      "Epoch 77/200\n",
      "17573/17573 [==============================] - 8s 464us/step - loss: 0.0908 - acc: 0.9776\n",
      "Epoch 78/200\n",
      "17573/17573 [==============================] - 8s 461us/step - loss: 0.0906 - acc: 0.9791\n",
      "Epoch 79/200\n",
      "17573/17573 [==============================] - 8s 460us/step - loss: 0.0868 - acc: 0.9804\n",
      "Epoch 80/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17573/17573 [==============================] - 8s 437us/step - loss: 0.0901 - acc: 0.9779\n",
      "Epoch 81/200\n",
      "17573/17573 [==============================] - 8s 437us/step - loss: 0.0984 - acc: 0.9747\n",
      "Epoch 82/200\n",
      "17573/17573 [==============================] - 8s 435us/step - loss: 0.0874 - acc: 0.9807\n",
      "Epoch 83/200\n",
      "17573/17573 [==============================] - 8s 436us/step - loss: 0.0848 - acc: 0.9788\n",
      "Epoch 84/200\n",
      "17573/17573 [==============================] - 8s 436us/step - loss: 0.0922 - acc: 0.9770\n",
      "Epoch 85/200\n",
      "17573/17573 [==============================] - 8s 436us/step - loss: 0.0876 - acc: 0.9772\n",
      "Epoch 86/200\n",
      "17573/17573 [==============================] - 8s 443us/step - loss: 0.0853 - acc: 0.9789\n",
      "Epoch 87/200\n",
      "17573/17573 [==============================] - 8s 451us/step - loss: 0.0856 - acc: 0.9798\n",
      "Epoch 88/200\n",
      "17573/17573 [==============================] - 8s 454us/step - loss: 0.0910 - acc: 0.9776\n",
      "Epoch 89/200\n",
      "17573/17573 [==============================] - 8s 450us/step - loss: 0.0878 - acc: 0.9784\n",
      "Epoch 90/200\n",
      "17573/17573 [==============================] - 8s 452us/step - loss: 0.0851 - acc: 0.9788\n",
      "Epoch 91/200\n",
      "17573/17573 [==============================] - 8s 452us/step - loss: 0.0895 - acc: 0.9780\n",
      "Epoch 92/200\n",
      "17573/17573 [==============================] - 8s 452us/step - loss: 0.0915 - acc: 0.9758\n",
      "Epoch 93/200\n",
      "17573/17573 [==============================] - 8s 449us/step - loss: 0.0845 - acc: 0.9808\n",
      "Epoch 94/200\n",
      "17573/17573 [==============================] - 8s 451us/step - loss: 0.0833 - acc: 0.9807\n",
      "Epoch 95/200\n",
      "17573/17573 [==============================] - 8s 450us/step - loss: 0.0900 - acc: 0.9767\n",
      "Epoch 96/200\n",
      "17573/17573 [==============================] - 8s 450us/step - loss: 0.0855 - acc: 0.9786\n",
      "Epoch 97/200\n",
      "17573/17573 [==============================] - 8s 447us/step - loss: 0.0844 - acc: 0.9808\n",
      "Epoch 98/200\n",
      "17573/17573 [==============================] - 8s 450us/step - loss: 0.0801 - acc: 0.9804\n",
      "Epoch 99/200\n",
      "17573/17573 [==============================] - 8s 451us/step - loss: 0.0863 - acc: 0.9775\n",
      "Epoch 100/200\n",
      "17573/17573 [==============================] - 8s 451us/step - loss: 0.0847 - acc: 0.9787\n",
      "Epoch 101/200\n",
      "17573/17573 [==============================] - 8s 449us/step - loss: 0.0804 - acc: 0.9810\n",
      "Epoch 102/200\n",
      "17573/17573 [==============================] - 8s 449us/step - loss: 0.0857 - acc: 0.9789\n",
      "Epoch 103/200\n",
      "17573/17573 [==============================] - 8s 449us/step - loss: 0.0874 - acc: 0.9787\n",
      "Epoch 104/200\n",
      "17573/17573 [==============================] - 8s 467us/step - loss: 0.0882 - acc: 0.9781\n",
      "Epoch 105/200\n",
      "17573/17573 [==============================] - 8s 457us/step - loss: 0.0860 - acc: 0.9781\n",
      "Epoch 106/200\n",
      "17573/17573 [==============================] - 8s 461us/step - loss: 0.0847 - acc: 0.9790\n",
      "Epoch 107/200\n",
      "17573/17573 [==============================] - 8s 453us/step - loss: 0.0842 - acc: 0.9780\n",
      "Epoch 108/200\n",
      "17573/17573 [==============================] - 8s 448us/step - loss: 0.0881 - acc: 0.9782\n",
      "Epoch 109/200\n",
      "17573/17573 [==============================] - 8s 451us/step - loss: 0.0868 - acc: 0.9787\n",
      "Epoch 110/200\n",
      "17573/17573 [==============================] - 8s 450us/step - loss: 0.0804 - acc: 0.9803\n",
      "Epoch 111/200\n",
      "17573/17573 [==============================] - 8s 453us/step - loss: 0.0840 - acc: 0.9780\n",
      "Epoch 112/200\n",
      "17573/17573 [==============================] - 8s 450us/step - loss: 0.0859 - acc: 0.9788\n",
      "Epoch 113/200\n",
      "17573/17573 [==============================] - 8s 453us/step - loss: 0.0789 - acc: 0.9812\n",
      "Epoch 114/200\n",
      "17573/17573 [==============================] - 8s 451us/step - loss: 0.0911 - acc: 0.9759\n",
      "Epoch 115/200\n",
      "17573/17573 [==============================] - 8s 449us/step - loss: 0.0868 - acc: 0.9772\n",
      "Epoch 116/200\n",
      "17573/17573 [==============================] - 8s 450us/step - loss: 0.0860 - acc: 0.9774\n",
      "Epoch 117/200\n",
      "17573/17573 [==============================] - 8s 450us/step - loss: 0.0852 - acc: 0.9784\n",
      "Epoch 118/200\n",
      "17573/17573 [==============================] - 8s 450us/step - loss: 0.0809 - acc: 0.9802\n",
      "Epoch 119/200\n",
      "17573/17573 [==============================] - 8s 455us/step - loss: 0.0814 - acc: 0.9794\n",
      "Epoch 120/200\n",
      "17573/17573 [==============================] - 8s 450us/step - loss: 0.0822 - acc: 0.9792\n",
      "Epoch 121/200\n",
      "17573/17573 [==============================] - 8s 449us/step - loss: 0.0833 - acc: 0.9781\n",
      "Epoch 122/200\n",
      "17573/17573 [==============================] - 8s 451us/step - loss: 0.0814 - acc: 0.9788\n",
      "Epoch 123/200\n",
      "17573/17573 [==============================] - 8s 452us/step - loss: 0.0836 - acc: 0.9783\n",
      "Epoch 124/200\n",
      "17573/17573 [==============================] - 8s 452us/step - loss: 0.0807 - acc: 0.9791\n",
      "Epoch 125/200\n",
      "17573/17573 [==============================] - 8s 452us/step - loss: 0.0908 - acc: 0.9752\n",
      "Epoch 126/200\n",
      "17573/17573 [==============================] - 8s 453us/step - loss: 0.0813 - acc: 0.9800\n",
      "Epoch 127/200\n",
      "17573/17573 [==============================] - 8s 451us/step - loss: 0.0799 - acc: 0.9793\n",
      "Epoch 128/200\n",
      "17573/17573 [==============================] - 8s 452us/step - loss: 0.0797 - acc: 0.9808\n",
      "Epoch 129/200\n",
      "17573/17573 [==============================] - 8s 453us/step - loss: 0.0793 - acc: 0.9796\n",
      "Epoch 130/200\n",
      "17573/17573 [==============================] - 8s 455us/step - loss: 0.0848 - acc: 0.9782\n",
      "Epoch 131/200\n",
      "17573/17573 [==============================] - 8s 458us/step - loss: 0.0780 - acc: 0.9803\n",
      "Epoch 132/200\n",
      "17573/17573 [==============================] - 8s 455us/step - loss: 0.0807 - acc: 0.9791\n",
      "Epoch 133/200\n",
      "17573/17573 [==============================] - 8s 456us/step - loss: 0.0824 - acc: 0.9791\n",
      "Epoch 134/200\n",
      "17573/17573 [==============================] - 8s 458us/step - loss: 0.0755 - acc: 0.9800\n",
      "Epoch 135/200\n",
      "17573/17573 [==============================] - 8s 454us/step - loss: 0.0841 - acc: 0.9774\n",
      "Epoch 136/200\n",
      "17573/17573 [==============================] - 8s 453us/step - loss: 0.0805 - acc: 0.9793\n",
      "Epoch 137/200\n",
      "17573/17573 [==============================] - 8s 456us/step - loss: 0.0816 - acc: 0.9807\n",
      "Epoch 138/200\n",
      "17573/17573 [==============================] - 8s 455us/step - loss: 0.0789 - acc: 0.9802\n",
      "Epoch 139/200\n",
      "17573/17573 [==============================] - 8s 457us/step - loss: 0.0827 - acc: 0.9800\n",
      "Epoch 140/200\n",
      "17573/17573 [==============================] - 8s 455us/step - loss: 0.0739 - acc: 0.9811\n",
      "Epoch 141/200\n",
      "17573/17573 [==============================] - 8s 454us/step - loss: 0.0777 - acc: 0.9811\n",
      "Epoch 142/200\n",
      "17573/17573 [==============================] - 8s 452us/step - loss: 0.0804 - acc: 0.9796\n",
      "Epoch 143/200\n",
      "17573/17573 [==============================] - 8s 461us/step - loss: 0.0796 - acc: 0.9788\n",
      "Epoch 144/200\n",
      "17573/17573 [==============================] - 8s 456us/step - loss: 0.0817 - acc: 0.9775\n",
      "Epoch 145/200\n",
      "17573/17573 [==============================] - 8s 456us/step - loss: 0.0810 - acc: 0.9796\n",
      "Epoch 146/200\n",
      "17573/17573 [==============================] - 8s 453us/step - loss: 0.0765 - acc: 0.9815\n",
      "Epoch 147/200\n",
      "17573/17573 [==============================] - 8s 454us/step - loss: 0.0809 - acc: 0.9784\n",
      "Epoch 148/200\n",
      "17573/17573 [==============================] - 8s 452us/step - loss: 0.0746 - acc: 0.9806\n",
      "Epoch 149/200\n",
      "17573/17573 [==============================] - 8s 458us/step - loss: 0.0765 - acc: 0.9811\n",
      "Epoch 150/200\n",
      "17573/17573 [==============================] - 8s 458us/step - loss: 0.0742 - acc: 0.9806\n",
      "Epoch 151/200\n",
      "17573/17573 [==============================] - 8s 456us/step - loss: 0.0836 - acc: 0.9782\n",
      "Epoch 152/200\n",
      "17573/17573 [==============================] - 8s 455us/step - loss: 0.0809 - acc: 0.9784\n",
      "Epoch 153/200\n",
      "17573/17573 [==============================] - 8s 455us/step - loss: 0.0751 - acc: 0.9800\n",
      "Epoch 154/200\n",
      "17573/17573 [==============================] - 8s 453us/step - loss: 0.0740 - acc: 0.9816\n",
      "Epoch 155/200\n",
      "17573/17573 [==============================] - 8s 454us/step - loss: 0.0784 - acc: 0.9814\n",
      "Epoch 156/200\n",
      "17573/17573 [==============================] - 8s 453us/step - loss: 0.0833 - acc: 0.9774\n",
      "Epoch 157/200\n",
      "17573/17573 [==============================] - 8s 455us/step - loss: 0.0808 - acc: 0.9775\n",
      "Epoch 158/200\n",
      "17573/17573 [==============================] - 8s 456us/step - loss: 0.0762 - acc: 0.9813\n",
      "Epoch 159/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17573/17573 [==============================] - 8s 447us/step - loss: 0.0748 - acc: 0.9804\n",
      "Epoch 160/200\n",
      "17573/17573 [==============================] - 8s 451us/step - loss: 0.0775 - acc: 0.9794\n",
      "Epoch 161/200\n",
      "17573/17573 [==============================] - 8s 450us/step - loss: 0.0765 - acc: 0.9807\n",
      "Epoch 162/200\n",
      "17573/17573 [==============================] - 8s 450us/step - loss: 0.0784 - acc: 0.9794\n",
      "Epoch 163/200\n",
      "17573/17573 [==============================] - 8s 448us/step - loss: 0.0823 - acc: 0.9781\n",
      "Epoch 164/200\n",
      "17573/17573 [==============================] - 8s 448us/step - loss: 0.0724 - acc: 0.9818\n",
      "Epoch 165/200\n",
      "17573/17573 [==============================] - 8s 449us/step - loss: 0.0733 - acc: 0.9820\n",
      "Epoch 166/200\n",
      "17573/17573 [==============================] - 8s 447us/step - loss: 0.0760 - acc: 0.9802\n",
      "Epoch 167/200\n",
      "17573/17573 [==============================] - 8s 447us/step - loss: 0.0767 - acc: 0.9802\n",
      "Epoch 168/200\n",
      "17573/17573 [==============================] - 8s 446us/step - loss: 0.0761 - acc: 0.9804\n",
      "Epoch 169/200\n",
      "17573/17573 [==============================] - 8s 447us/step - loss: 0.0733 - acc: 0.9814\n",
      "Epoch 170/200\n",
      "17573/17573 [==============================] - 8s 447us/step - loss: 0.0718 - acc: 0.9808\n",
      "Epoch 171/200\n",
      "17573/17573 [==============================] - 8s 446us/step - loss: 0.0725 - acc: 0.9805\n",
      "Epoch 172/200\n",
      "17573/17573 [==============================] - 8s 447us/step - loss: 0.0758 - acc: 0.9794\n",
      "Epoch 173/200\n",
      "17573/17573 [==============================] - 8s 446us/step - loss: 0.0665 - acc: 0.9828\n",
      "Epoch 174/200\n",
      "17573/17573 [==============================] - 8s 447us/step - loss: 0.0716 - acc: 0.9817\n",
      "Epoch 175/200\n",
      "17573/17573 [==============================] - 8s 448us/step - loss: 0.0786 - acc: 0.9791\n",
      "Epoch 176/200\n",
      "17573/17573 [==============================] - 8s 447us/step - loss: 0.0737 - acc: 0.9814\n",
      "Epoch 177/200\n",
      "17573/17573 [==============================] - 8s 445us/step - loss: 0.0748 - acc: 0.9805\n",
      "Epoch 178/200\n",
      "17573/17573 [==============================] - 8s 445us/step - loss: 0.0740 - acc: 0.9822\n",
      "Epoch 179/200\n",
      "17573/17573 [==============================] - 8s 449us/step - loss: 0.0719 - acc: 0.9814\n",
      "Epoch 180/200\n",
      "17573/17573 [==============================] - 8s 448us/step - loss: 0.0700 - acc: 0.9823\n",
      "Epoch 181/200\n",
      "17573/17573 [==============================] - 8s 446us/step - loss: 0.0779 - acc: 0.9788\n",
      "Epoch 182/200\n",
      "17573/17573 [==============================] - 8s 448us/step - loss: 0.0821 - acc: 0.9784\n",
      "Epoch 183/200\n",
      "17573/17573 [==============================] - 8s 447us/step - loss: 0.0729 - acc: 0.9809\n",
      "Epoch 184/200\n",
      "17573/17573 [==============================] - 8s 445us/step - loss: 0.0722 - acc: 0.9805\n",
      "Epoch 185/200\n",
      "17573/17573 [==============================] - 8s 448us/step - loss: 0.0686 - acc: 0.9813\n",
      "Epoch 186/200\n",
      "17573/17573 [==============================] - 8s 444us/step - loss: 0.0703 - acc: 0.9821\n",
      "Epoch 187/200\n",
      "17573/17573 [==============================] - 8s 446us/step - loss: 0.0762 - acc: 0.9797\n",
      "Epoch 188/200\n",
      "17573/17573 [==============================] - 8s 446us/step - loss: 0.0707 - acc: 0.9822\n",
      "Epoch 189/200\n",
      "17573/17573 [==============================] - 8s 445us/step - loss: 0.0798 - acc: 0.9791\n",
      "Epoch 190/200\n",
      "17573/17573 [==============================] - 8s 448us/step - loss: 0.0775 - acc: 0.9778\n",
      "Epoch 191/200\n",
      "17573/17573 [==============================] - 8s 446us/step - loss: 0.0681 - acc: 0.9829\n",
      "Epoch 192/200\n",
      "17573/17573 [==============================] - 9s 514us/step - loss: 0.0687 - acc: 0.9833\n",
      "Epoch 193/200\n",
      "17573/17573 [==============================] - 9s 500us/step - loss: 0.0714 - acc: 0.9812\n",
      "Epoch 194/200\n",
      "17573/17573 [==============================] - 11s 649us/step - loss: 0.0772 - acc: 0.9800\n",
      "Epoch 195/200\n",
      "17573/17573 [==============================] - 9s 521us/step - loss: 0.0673 - acc: 0.9826\n",
      "Epoch 196/200\n",
      "17573/17573 [==============================] - 11s 606us/step - loss: 0.0766 - acc: 0.9796\n",
      "Epoch 197/200\n",
      "17573/17573 [==============================] - 11s 632us/step - loss: 0.0735 - acc: 0.9809\n",
      "Epoch 198/200\n",
      "17573/17573 [==============================] - 11s 600us/step - loss: 0.0720 - acc: 0.9801\n",
      "Epoch 199/200\n",
      "17573/17573 [==============================] - 10s 592us/step - loss: 0.0753 - acc: 0.9798\n",
      "Epoch 200/200\n",
      "17573/17573 [==============================] - 8s 471us/step - loss: 0.0763 - acc: 0.9798\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f46f816dda0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_x, train_y, epochs = 200, batch_size = 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert a line to one hot form\n",
    "def convert_line_to_one_hot(line, letter_int):\n",
    "    output = []\n",
    "    for letter in line:\n",
    "        output.append(get_word_repr(letter_int, letter))\n",
    "    return np.array(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert from one hot to letter\n",
    "def vect_to_letter(vect, int_letter):\n",
    "    max_index = 0\n",
    "    max_value = 0\n",
    "    for curr_index, curr_value in enumerate(vect):\n",
    "        if max_value < curr_value:\n",
    "            max_index = curr_index\n",
    "            max_value = curr_value\n",
    "    return int_letter[max_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a RNN generates a shakespeare sonnet\n",
    "def generate_poem(RNN, letter_int, int_letter):\n",
    "    # Initial line to seed \n",
    "    first_line = \"shall i compare thee to a summer\\'s day?\\n\"\n",
    "    \n",
    "    curr_line = convert_line_to_one_hot(first_line, letter_int)\n",
    "    curr_poem = first_line\n",
    "    line_num = 2\n",
    "    while line_num < 14:\n",
    "        # Predict the next character\n",
    "        next_char = RNN.predict(np.array([curr_line]))[0]\n",
    "        # Update curr_line and curr_poem\n",
    "        curr_poem += vect_to_letter(next_char, int_letter)\n",
    "        if vect_to_letter(next_char, int_letter) == \"\\n\":\n",
    "            line_num += 1\n",
    "        curr_line = convert_line_to_one_hot(curr_poem[-40:], letter_int)\n",
    "    \n",
    "    return curr_poem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shall i compare thee to a summer's day?\n",
      "the io al ond rovel this hight praceed no cooke.\n",
      "sow ceme thy cort, wor with fratt far love,\n",
      "and yout my send and all the arpound,\n",
      "and that thenedest tor, wellds pormous toudd.\n",
      "be'ing nit eve shuch cone that brosed no,\n",
      "and and that is on ollle's sprice,\n",
      "and so grack and theis farture as with me,\n",
      "the fir thene shace as end thy hrowed,\n",
      "that i hournele sucround of decigeincy ing.\n",
      "art;\n",
      "e cornul abk im monese whan thy vesert,\n",
      "pood to thee wre whech the forr would sulf can,\n",
      "\n"
     ]
    }
   ],
   "source": [
    "poem = generate_poem(model, letter_int, int_letter)\n",
    "print(poem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'h'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
